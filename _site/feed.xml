<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mapping With Matt</title>
    <description>Portfolio built with Barber, a blog theme for Jekyll built by Thomas Vaeth for Samesies using HTML, Sass, and JavaScript.
</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 06 Oct 2020 21:50:12 -0700</pubDate>
    <lastBuildDate>Tue, 06 Oct 2020 21:50:12 -0700</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Revitilization of a Sanborn Map: Digitizing Hollywood District</title>
        <description>&lt;h3 id=&quot;intro&quot;&gt;Intro:&lt;/h3&gt;

&lt;p&gt;This project illustrates the reconstruction of several street blocks of Hollywood in the 1950s through the use of digitizing and vector editing. A blueprint of a 1950 Sanborn fire insurance map was used as the template to create the digitized, print-quality map. The past footprints of the Sanborn map are transferred to its digitized counterpart with visually categorized building types. For those unfamiliar with Hollywood, the northwestern corner of the area represented in the georeferenced GeoTIFF is the intersection between Hollywood Blvd and Cahuenga.&lt;/p&gt;

&lt;h3 id=&quot;1950-sanborn-map---a-relic-of-the-past&quot;&gt;1950 Sanborn Map - A Relic of the Past&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sanborn.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hollywood-district---a-digitized-restoration&quot;&gt;Hollywood District - A Digitized Restoration&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/digitizing.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Aug 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/08/25/digitizing/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/08/25/digitizing/</guid>
        
        <category>Digitizing</category>
        
        
      </item>
    
      <item>
        <title>LA County Criminal Mappings: Heat Mapping &amp; Thiessen Polygons of Reported Crimes</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;To solve questions or ambiguities around proximity and predictive policing, various methods such as density maps and heat mappings are curated for distribution and analysis purposes. This project uses heat maps to illustrate crimes of the highest magnitude in Los Angeles County: Grand Theft Auto, Possession of Narcotics, and Driving Under the Influence. Maps that accentuate the need for predictive policing were curated through the addition of Thiessen Polygons - an analysis that focuses on proximity of crimes around police stations in Los Angeles County.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;Data for crime rates (and classification of crimes) was provided by the Los Angeles County Sheriff’s department. This data was imported as a CSV into Arcmap as X,Y data and took the form of ‘crime points’ on the maps. These points were re-projected using ‘CA State Plane V’ for correct placing. ArcToolbox’s density tool allowed for the computation of kernel density .tif’s for each crime. These were overlaid on an unobtrusive, light-grey basemap. This process was done for each crime under investigation.&lt;/p&gt;

&lt;p&gt;The last two maps contain the same crime data - Grand Theft Auto, Possession of Narcotics, and Driving Under the Influence - but focus on predictive policing. A shapefile containing the amount of police stations within Los Angeles County was downloaded. Thiessen polygons around each police station were produced using the ‘Analysis Proximity’ feature in ArcToolbox. This creation allowed for an illustration of which zones were within closest proximity for the given police station’s response. Spatially joining the crimes with Thiessen polygons assessed the total number of crimes within each polygon. Through this, an analysis of which police station could use aided help or more resources available to reduce crime. The polygons were color schemed by lowest to highest density of crimes. The level of scale differentiates these two maps; the first map exemplifies Thiessen polygons around each station, while the last map illustrates the number of crimes per census tract in LA County. The census tracts have a color scheme denoting which tracts have the highest concentration of crime, alluding to which areas require a larger concentration of officers patrolling.&lt;/p&gt;

&lt;p&gt;Areas of improvement on the maps below can be seen in the count breakdown portion of the Legend. Instead of using numbers for the first three mappings - the heat maps representing specific crimes - a more informative breakdown should be presented (like ‘Low,’ ‘Moderate,’ and ‘High’). The ‘Count’ breakdown for the last two maps should be something more symbolic: like ‘Number of Crimes Committed.’&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Narcotics.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTA.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/DUI.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Crimes-Thiessen.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Crime-per-tract.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 28 Aug 2019 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2019/08/28/thiessen-mapping/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/28/thiessen-mapping/</guid>
        
        <category>Density Mapping</category>
        
        
      </item>
    
      <item>
        <title>The Spatial Dynamics of Presidential Elections: An Autocorrelation Analysis of Voter Behavior in 2012 &amp; 2016</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This project exemplifies an advanced method of interpolation mapping, through the lens of spatial autocorrelation between voting characteristics of the 2012 and 2016 Presidential elections. By subtracting the Democratic vote shares between the years 2012 and 2016, a change in voter behaviors is made tangible - proven by a strong, positive correlation of 0.644 produced through GeoDa (chart 3). This positive correlation denotes the influence of spatially clustered locations (counties) on voting characteristics in the United States. 64% of Democratic voting share change can be assumed from the spatial clustering of linked county surroundings.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Voter Behaviors (2012):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pres-2012.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Moran-2012.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This clustered illustration of global county votes underscores a strong spatial autocorrelation (0.617) between voting patterns in the United States. This value was produced vis-à-vis the software GeoDa and was held by a ‘First Order Queen Contiguity’ basis. When comparing this value to the below chart of 2016 (0.602), a slight decrease in the spatial clustering of voting characteristics in the United States is realized.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Voter Behaviors (2016):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pres-2016.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Moran-2016.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The emphasis of a county-level scale of vision allowed for a more nuanced analysis of spatial patterns in voting behaviors. Specifically, the voting characteristics of Democratic voters in the United States of the primary election between 2012 and 2016. The largest clustering of Democratic voting counties in 2012 are those within the states of the West Coast boundary (California and Washington), as well as counties comprising the Midwest and Northeastern divisions. 4 years later (in 2016), the clustering of Democratic voters was reduced in both the Midwest region and Northeast regions. Paradoxically, states like California and Washington received an increase in Democratic votes in the year 2016.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Change in Voter Behaviors:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/change-in-pres.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Moran-change.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By analyzing the percent change in Democratic voting share characteristics between 2012 to 2016, a consensus can be drawn: The greatest clustering of Democratic vote shares are almost entirely comprised of the US coastlines and the South West. This is interesting due to the nature of Southern states being predominately Republican. And yet, although the Democratic party most certainly gains from this clustering between coastlines and the Southern region, it equally lost votes in the Northeastern and Midwest boundaries.&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Sep 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2018/09/16/Presidential-Election-Autocorrelation/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/09/16/Presidential-Election-Autocorrelation/</guid>
        
        <category>Interpolation</category>
        
        
      </item>
    
      <item>
        <title>Precipitation Calculations of Ski Resorts in Placer County, CA</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This project capitalizes on two core concepts - Interpolation (Kriging) and the multi-layered process of building a least-cost path. The study of interest is Placer County (North Shore Lake Tahoe), known for its popularity as a snowboarding/ski resort. Precipitation patterns in the months of January and August - typically California’s coldest and hottest months - are cross-referenced with the distance traveled among Tahoe’s most common ski resorts: Homewood, Alpine, Squaw, and NorthStar. Tahoma Meadows, another ski resort, was chosen as the origin for the path analysis portion of the study.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;The first two maps illustrating the precipitation values in Placer County showcase the effectiveness of Kriging interpolation for the months of January and August. Data for precipitation values were garnered through NOAA, and then added as XY data points in ArcMap.&lt;/p&gt;

&lt;p&gt;The three-dimensional rendering of the path analysis between ski resorts in Placer County, demonstrates the most affordable path from an origin location (Tahoma Meadows), to four counter destination locations (the 4 listed ski resorts).&lt;/p&gt;

&lt;p&gt;Considerations towards the added precipitation factor (the mean of both months divided by 2), slope, and bodies of water were accounted for. A total of 4 Digital Elevation Models were downloaded for the county of Placer and combined into a single TIFF. A calculation for slope interference was then performed. Slope was reclassified between two values (old and new), with ranges between 0 - 60 and 1 - 512 accordingly. The effect of water was  highlighted through a raster calculation — set with a score less than or equal to 1. Water was present in areas that assessed a score of ‘1’ and absent in places receiving a score of ‘0.’ Another raster classification was performed, this time incorporating the newly added presence of water and slope. The original slope was added to the total amount of water and then multiplied by 128.&lt;/p&gt;

&lt;p&gt;The ski resort locations were downloaded through the website ‘Click2shp.’ After calculating the cost distance between the locations, the quickest path between each resort was realized. Tahoma Meadows was chosen as the origin due to its placement with Homewood - the shortest distance between any ski resort in Placer County. This process was repeated three more times (for the remaining three ski resorts). The three-dimensional representation of this final product was performed in ArcScene. Adding an elevation gradient and altering the map’s Base Heights minimized the impact of Lake Tahoe and prioritized the path as the map’s main focus. Accentuation of detail was produced by setting the total elevation to 5, offsetting each layer by 50°, and adding lighting for visible contrast.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Precipitation Values: Placer County, CA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/January-Interpolation.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/August-Interpolation.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3D Rendering of Path Analysis to Ski Resorts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3d-Placer.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Aug 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2018/08/27/Interpolation-Placer/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/08/27/Interpolation-Placer/</guid>
        
        <category>Interpolation</category>
        
        
      </item>
    
      <item>
        <title>Statewide Precipitation Averages: California</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;The analysis of statewide precipitation averages between two extremes - January (high) and August (low) - allows for an examination between two forms of Interpolation: IDW and Kriging. These differing methods of simulation grant a better understanding of how the same data values can be altered through representation. This project also highlights California’s Mediterranean Climate; the state experiences its peak temperatures during the summer months (like August), and vast periods of coolness during its winter months (January).&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;Temperature values in California for the months of January and August were accessed from NOAA. The provided CSV was then added into ArcGIS as XY data. The IDW interpolation method and Kriging interpolation method were then used to see which simulation did a better job at classifying the state’s precipitation ranges.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;IDW Interpolation Mapping:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/january-interp-ca.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/California-August-Interpolation.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kriging Interpolation Mapping:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/kriging-august.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h3&gt;

&lt;p&gt;This study allowed for juxtaposing months’ temperature values in the state of California to be witnessed. The temperature range of January was between 36°F and 72°F, while the range for the month of August was between 62°F and 114°F. January’s lowest average temperature was in Sunnyside-Tahoe City (39°F), and the month’s highest average temperature was in Mecca (72°F). This follows the trend of Northern California having a cooler climate than that of Southern California, where the contrasting cities reside. August’s lowest average temperature was in Westhaven-Moonstone (63°F), and the month’s highest average temperature was in Furnace Creek (114°F).&lt;/p&gt;

&lt;p&gt;The first two maps utilize the IDW interpolation method to create a quantile-based representation of temperature. In my opinion, the final map using the Kriging Bayesian method is more suitable for representation. When switching the Kriging method, the range of temperature in the month of August was adjusted from 62°F to 59°F. Although a change in 3°F may seem miniscule, it has huge implications with regards to climate change. The distribution of temperatures also appears less rigid and lends itself to a cleaner representation.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Aug 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2018/08/26/Interpolation-CA/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/08/26/Interpolation-CA/</guid>
        
        <category>Interpolation</category>
        
        
      </item>
    
      <item>
        <title>US Mass Shootings: Forces of Prediction, or Varying Inconsistencies?</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;The purpose of this project is to investigate the regularity and development of a national concern at large: Mass Shootings. Mass shootings are defined as shootings with 3 or more casualties, not including the shooter. This report will analyze the emergence of mass shootings in the United States — beginning with the first reported mass shooting in 1966 — to create a streamlined sequence of the progression of mass shootings. With a palpable timeline created, classifications of patterns or inconsistencies surrounding a periodically deconstructed set of mass shootings emerges to the surface. The varying degrees of facets entwined in mass shootings (predominately the characteristics of each shooter), coupled with the physical distribution of each recorded mass shooting in the United States, allows for a scale of predictability of shooting likeliness to further transpire. Because mass shootings entail a broad spectrum of place occurrence — such as residential neighborhoods, places of worship, government facilities, etc. — I decided to further focus on a nuanced component of contemporary concern: School Shootings. By cross-referencing the distribution of school shootings to mass shootings in the United States, this time with periodically structured dates (pre-2000, 2000 to 2010, and 2010 to 2015), a more precise analysis of shooting occurrence is achieved. Lastly, this report will take into consideration another contemporary topic under heavy debate: the effectiveness of state gun laws to combat mass shootings. Through the comparison of state shooting frequencies to state rankings in gun laws, yet another understanding of the distribution of mass shootings can be inferred. It should be stated early on that perfectly predicting such sporadic events like mass shootings is nearly impossible. Although density is an effective form of analysis, these shootings are often catalyzed by the offset of a shooter’s mental instability and are thus unpredictable.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Map 1 - Total United States Mass Shootings:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With the data now placed in the correct latitudinal and longitudinal locations and projected in a coordinate system that offered a less stressed representation of the United States (NAD 1927 Contiguous USA Albers) to show a better demonstration of mass shooting clusters, graduated symbols were then applied for each shooting. The symbol size ranged from 10 to 30, with the lowest total fatality rate ranked 0 to 7 given a symbol size of 10, and the largest total fatality ranked 27 to 33 given a symbol size of 30. The transparency was set to 15% to allow for all symbols to be seen — 334 mass shootings are displayed as varying sized red symbols on the map, many of which cover one another. A kernel density for total mass shootings was then applied. A color scheme ranging from green to red (green being the lowest density and red being the most concentrated) was used. This raster was clipped to the bounds of a US boundary shapefile (hollow to preserve the raster’s bright color and allow the ESRI Dark Gray Canvas Basemap to contrast in full effect). Because the raster was simply for accentuating the density of mass shootings, I didn’t deem it necessary to include the High to Low values on the map itself. However, for the sake of analysis, the raster had a low value of 0 and a high value of 2.45315. Calculation and summary statistics were used within the attribute table to gather the total number of mass shootings, and total number of fatalities per shooting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Map 2 - United States School Shootings by Year:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This map was produced following several of the same geoprocessing and overlay techniques as the mass shooting map. However, multiple query builder simulations were used to isolate school shootings, as well as shootings that occurred within the given data parameters I assigned. I then extracted these features from the data layer and added them as new shapefiles. The three-time breakdowns each were assigned a specific coloring (1966 to 1999 in blue, 2000 to 2010 in yellow, and 2010 to 2015 in green), and graduated symbols were applied to each layer to have each point sized to the total fatality rate. I then combined these school shooting data layers into a single layer with the geoprocessing merge technique and created another new shapefile. A kernel density of school shootings within the United States with the same color scheme of green to red was applied to the merged school shooting layer. This allowed for the same illustration of density but showcased a different representation of shootings — this time school related instead of mass shootings as a whole. Because there are less data points (54 school shootings as opposed to 334 mass shootings), there is less of a transition from low to high (green to yellow to red) on the map. Again, because the kernel density was for added for extra attention for density, I didn’t deem showing the raster values necessary on the map. However, for the sake of analysis, the raster had a low value of 0.000501944 and a high value of 0.198501. Calculation and summary statistics were used within the attribute table of each layer to gather the total number of school shootings per timeline, and total number of fatalities per shooting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Map 3 - State Gun Law Score vs State Shooting Occurrence:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While this map followed the same procedures as the previous two, most of the data was able to come to fruition through the process of adding fields and editing attribute values. I added two fields to the original mass shooting layer: State Gun Law Score, and State Shooting Fatalities. For the state gun law score, I manually reclassified the scores (originally alphabetically A to F) to a score of 1 to 5 (1 representing ‘A’ and 5 representing ‘F’). I merged this newly edited layer to the US boundary layer. To illustrate each state’s gun law ranking, a graduated color scheme was applied for this variable. The reclassified values denoted the range for each state’s extent — a range of 1 being the lightest blue, and a range of 5 being the darkest blue. I exported the layer as a new shapefile and re-uploaded it, this time focusing on state shooting fatalities. I used the same process of graduated symbology (the value being the newly added state shooting field), with a range of 10 to 35. States within the smallest range (0 to 8) were given a symbol size of 10, and states within the largest range (33 to 40) were given a symbol size of 35. The added size allowed for a better visual of state shooting fatality total to be captured.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Geoda - Spatial Autocorrelation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The use of Geoda was to calculate a sense of spatial autocorrelation between a state’s gun law score, to the amount of shootings that have occurred within each state. To calculate the degree of correlation, I held state shootings as the dependent variable, and state gun law scores as the independent variable, and produced a scatterplot. A negative slope (-0.071) indicated a negative relationship to two variables, and therefor little to no correlation. I used Univariate Local Moran’s 1 and created weights for the states Poly_ID with Queen Contiguity. I set the order of contiguity to 1 and allowed for K-nearest neighbors to be highlighted. Again, using Univariate Local Moran’s 1, I set the first variable X to state gun law score and produced a significance map, cluster map, and Moran scatterplot. This showed which states had clusters, and which were neighborless. Setting 999 permutations, and a significance filter of 0.01, showed the spatial autocorrelation value — in terms of not significant, high-high, low-low, low-high, high-low, and neighborless.&lt;/p&gt;

&lt;h3 id=&quot;mapping-mass-shootings-over-time---do-state-gun-laws-matter&quot;&gt;Mapping Mass Shootings Over Time - Do State Gun Laws Matter?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/total-mass-shootings.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map illustrates the full summation of mass shootings in the United States, from 1966 to 2015. While a mass shooting is denoted by fatalities greater than 3, I have incorporated all shootings with a total injury status of greater than or equal to 3. However, in an effort to maintain consistency regarding units with the following maps, I calculated statistics for total fatalities of US mass shootings (not including total injuries). By incorporating those injured with those killed, the scope of mass shootings is greatly widened; this is of great significance when predicting the likeliness of a shooting, because tracking attempted mass shootings allows for a future projection of shooting locations. Within 49 years, there have been 334 mass shootings in the United States. Statistically, this would mean on average there have been 7 mass shootings for every given year from 1966 to 2015. Throughout the many 334 mass shootings, a total of 1,352 people have lost their lives — an average of 4 fatalities per mass shooting.&lt;/p&gt;

&lt;p&gt;The colored density of the map showcases the intensity of total fatalities per mass shooting: green illustrates the least number of people killed, while yellow to orange denotes a moderate to high fatality recording, and red exemplifying the largest proportion of lives lost. The United States northern and central boundaries — states like Utah, Wyoming, Montana, North and South Dakota — have the lowest recording of mass shootings, and therefore the lowest proportion of stimulated fatalities. This is in stark contrast with the southern states and states governing the nation’s East Coast. States like Tennessee, Georgia, North and South Carolina, Kentucky, Virginia, Indiana, Ohio, Pennsylvania, Delaware, and Maine constitute the highest density of mass shootings in the United States. However, the state of California is the only state on the West Coast with a similar density of mass shootings (predominately the state’s southern border) and stands as an outlier to this southeastern clustering. Blacksburg, Virginia had the highest proportion of total fatalities (33) at Virginia Tech campus in 2007, followed by Newton, Connecticut (28) at the Sandy Hook Elementary School in 2012, and Killeen, Texas (24) at Luby’s California restaurant in 1991. California’s largest mass shooting occurred at a McDonald’s restaurant in San Ysidro in 1984 with 22 fatalities. In terms of bulk mass shooting occurrence, California ranked highest with 40 mass shootings, followed by Texas with 23 and Florida with 25.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/School-Shootings-By-Year.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map focuses on a nuanced representation within the grand scheme of mass shootings: School Shootings. I selected school shootings to embody a more specific embodiment of mass shooting patterns, because in a contemporary setting such as 2018, school shootings are thought as rising regularities and normalized occurrences. By deconstructing school shooting incidents into 3 classifications — pre-2000 (1966 to 1999), 2000 to 2010, and 2010 to 2015 — a three-part understanding of school shooting development within the United States is able to be evaluated. Collectively, there have been 54 school shootings within the United states from the parameters of 1966 to 2015. On average, 5 people were killed in every shooting. Over the course of 30 years, from the first school shooting in 1966 to the last school shooting in 1999, there were 54 occurrences. While this chronology is three times the duration of the latter two deconstructions, it had nearly six times the average school shooting rate (40 shootings occurred from 1966 to 1999). Amongst those 54 school shootings, there were a total of 262 fatalities and an average of 5 lives lost per shooting. The most school shootings occurred in the state of California (7), followed by the state of Illinois (3).&lt;/p&gt;

&lt;p&gt;The first school shooting at the University of Texas in 1966 denotes the largest shooting within the timeline; 17 individuals were killed. The third largest fatality rate induced by a school shooting occurred 10 years later in 1976 at Cal State Fullerton, where 7 people lost their lives. The second largest school shooting occurred in the last year of the given parameters (1999), in Littleton, Colorado at Columbine High School, where 14 people were tragically killed. Over the course of the next 10 years (between 2000 to 2010), there were 7 incidents. Between 2000 to 2010, there were 76 total fatalities stimulated by school shootings, with an average of 11 fatalities per shooting. The largest fatality rates were among the years 2005, 2007, and 2009. In 2005, in Red Lake, Minnesota, 10 people were killed at Red Lake High School. Two years later (2007), the largest school shooting within the timeline — and the largest school shooting in US history — occurred in Blacksburg, Virginia with a record high fatality rate of 33 lives lost at Virginia Tech. Just another two years later (2009), the second largest school shooting within the United States went underway: 14 people were killed in Binghamton, New York. The shortest and most contemporary timeline (between 2010 and 2015), followed the same trajectory of total school shootings as 2000 to 2010 with 7 incidents. However, there were 13 fewer total fatalities (63), and a lower average fertility of 9 lives lost per shooting. The first school shooting within this timeline was in Oakland, California (2012) at Oikos University with 7 fatalities. The largest school shooting between 2010 and 2015 was in the same year (2012) in Newton, Connecticut at Sandy Hook Elementary School; a total of 28 children and faculty members lost their lives. The next largest school shooting occurred 3 years later in Roseburg, Oregon (2015) at Umpqua Community College, where 10 fatalities were catalyzed by a school shooting.&lt;/p&gt;

&lt;p&gt;Out of the total 334 mass shootings that have occurred in the United States, 6% of these were school shootings (54). While focusing on a nuanced category like school shootings offers a new perspective on patterns of distribution, it still falls under the same umbrella of mass shootings and therefore is just as volatile in terms of prediction. As illustrated on the map, the distribution of school shootings is pretty balanced, with much fewer concentrations of clusters like that of total United States mass shootings (map 1). However, the largest fatality rates caused by school shootings does follow the mass shooting generalization of targeting states near the East Coast (highlighted in red), where upwards of 30 school shootings have taken place. While school shootings are becoming thought of as rising trends of regularity, it actually appears quite the opposite; over the last 20 years there were a total of 14 school shootings where an individual was killed. This is in stark contrast to the 40 school shootings between the years 1966 to 1999.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Gun-Law-vs-Shooting-Occurence.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Categorizing states by their gun law score, in comparison to each state’s total fatality rate from mass shootings, allows for a range of law effectiveness to merge at a national level. A state’s gun law score is classified on a scale from A to F, with a score of ‘A’ being the highest and a score of F being the lowest. I re-classified these as scores of 1 (A) to 5 (F). Exactly half of the United States ranked the lowest possible score of 5 (25 states). Tennessee, Virginia, Indiana, North Carolina, Ohio, and Nevada (6 states) scored a ranking of 4. Minnesota, Iowa, Michigan, Oregon, Wisconsin, Colorado, and Pennsylvania (7 states) scored a ranking of 3. Delaware, Illinois, Rhode Island, and Washington (4 states) scored a ranking of 2. California, the District of Columbia, Maryland, New York, Massachusetts, Connecticut, and New Jersey (7 states) are ranked with the highest possible gun law score of 1. These gun law classifications are illustrated by varying shades of blue on the map — with the lightest blue representing the highest ranking, and the darkest shade of blue denoting the lowest ranking. The analysis of mass shootings was discussed extensively in map 1; however, whereas the first map of United States mass shootings simply showcases the total number of shootings and their given location, this map illustrates the total volume of mass shootings for each state by fatality rate. Each state’s fatality rate is represented by a red symbol of varying sizes; the larger the red symbol, the higher the proportion of lives lost to mass shootings. The bulk quantity of states (36) have experienced fewer than 8 total fatalities from mass shootings. A total of 12 states have experienced between 9 to 16 total fatalities from mass shootings. It makes sense that the top three states with the highest proportion of mass shooting (Texas, Florida, and California), would also have the largest proportion of total fatalities. Texas has experienced 23 total fatalities (falling within the range of 17 to 24 on the map), just one fatality rate higher than Florida (25), falling within the range of 25 to 32 on the map. California has experienced the most causalities from mass shootings (40), falling within the range of 33 to 40 on the map and having the largest red dot. This map allows for highly debated controversies over gun laws to emerge. For instance, California is classified as having one the strictest gun law scores (1), and yet has the highest total fatality rate from mass shootings than any other state in the US. New York also received a gun law score of 1, and yet has a total of 12 fatalities induced from mass shootings — just one less fatality than states like Kansas and Arizona, both of which are classified by the lowest gun law ranking of 5. Similarly, the state of Washington is characterized by a gun law ranking of 2, and yet has the same proportion of total fatalities from mass shootings (15) as the state of Georgia, which has a ranking of 5. This analysis leads to believe that a state’s degree of gun law ranking appears to have little correlation with total fatality rate from a mass shooting.&lt;/p&gt;

&lt;h3 id=&quot;geoda-state-shooting-occurrence-vs-gun-law-score&quot;&gt;Geoda: State Shooting Occurrence vs. Gun Law Score&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Geoda-Scatterplot.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to verify this lack of correlation, a statistical analysis between a state’s gun law score and the total fatalities induced by mass shootings was needed. The state’s gun law scores were held as the independent variable, and the number of mass shootings within each state were held as the dependent variable. With the given scatterplot, the slope b (correlation coefficient) illustrates a negative relationship (-0.071) between the two variables. To put this in terms of representation on the entire United States, a significance filter of 0.1 was applied, and clusters with rankings of 0 to 5 for each state’s gun law ranking were achieved. A total of 40 states received a cluster ranking of not significant, showing little to no positive spatial autocorrelation.&lt;/p&gt;

</description>
        <pubDate>Thu, 31 Aug 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2017/08/31/mass-shootings/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/31/mass-shootings/</guid>
        
        <category>Raster Analysis</category>
        
        
      </item>
    
      <item>
        <title>Site Suitability Analysis: San Diego Elevation Mappings for a Summer Campground</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This project delves into the complexities of site suitability analyses, through the acquisition of a DEM (10m resolution) and ground cover vegetation for the highest mountain range in Eastern San Diego County - Laguna and Cuyamaca Mountains. Several rasters were used to systematically identify the most optimal camping locations, and then reclassified using map algebra tools to meet the requested requirements for campsite locations (listed below). The process of selecting a campground site was facilitated through the development of a terrain-based scoring index (out of 10 points), wherein locations receiving higher scores are better suited for use as development areas of campgrounds.&lt;/p&gt;

&lt;p&gt;Improvements on the produced maps could be made on representation of the legend - the resolution of the export process compromises the understanding of elevation in the first map. The legend for the second map (San Diego Camping Locations) only needs to include a single symbol representing all camping locations that meet the required parameters.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;The parameters set for requested campsite locations were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An elevation equal or greater than 4,000 feet above sea level to ensure summer temperatures will be cool enough for comfortable camping.&lt;/li&gt;
  &lt;li&gt;Within two miles of an existing road.&lt;/li&gt;
  &lt;li&gt;Vegetation cover with one of the following classifications: Coniferous Forest, Oak Woodlands, or Mixed Forests.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rasters were reclassified using the following point values:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/terrain-elevation.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/terrain-slope.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/san-diego-dem.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/camping-locations.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Aug 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2017/08/30/San-Diego/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/30/San-Diego/</guid>
        
        <category>Raster Analysis</category>
        
        
      </item>
    
      <item>
        <title>Modifications of T-Mobile Cell Towers for Accentuated Coverage Areas: Los Angeles, CA</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This study epitomizes the capabilities of advanced methods of raster analysis - a multi-faceted approach of calculating the coverage of T-Mobile cell towers in LA County through the alteration of various scenarios. Likelihood of enhancements in cell coverage were thought through 3 possibilities: adding cell towers, expanding the current range of coverage, or increasing the height of current cell towers. The success of this project fringed upon Viewshed Analysis, which allowed for these hypothetic adjustments to T-Mobile’s cell towers to come to fruition.&lt;/p&gt;

&lt;p&gt;Improvements could be made by showcasing the increased range in cell coverage in a different color than the current range. Centering the area of study (LA County) to the map extent and minimizing map features (North Arrow, Scale, and Legend), would produce a more visually appealing map from a cartographic point-of-view. Illustrating the addition of towers in the second map could also be more apparent with a different representation that’s easier to notice.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;The outline of LA County was provided from USGS EarthExplorer as an array of GeoTIFF’s (digital elevation models). These were then converted into rasters, through the “Mosaic to New Raster” option in ArcToolbox. The specific locations for each cell tower were obtained through a CSV, downloaded on T-Mobile’s website. This CSV was then converted into a shapefile, with its attribute table being edited to meet each required element — T-Mobile’s current coverage (map 1), coverage with the addition of 3 towers (map 2), coverage with an increased range of 5,000Km (map 3), and coverage with increased tower heights of 10m (map 4).&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/t-mobile-cell-coverage.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map illustrates the current placement of T-Mobile towers in LA County. In total, there are 104 cell towers - the majority of which are concentrated in central and southern Los Angeles. The towers’ height, coverage radius, and several factors about the earth’s trajectory were set at given increments. The total amount of area not covered in Los Angeles County is 58.9%, an average calculated by dividing the maximum (7,247,374,591) by the sum (12,301,062,346). To find the average area covered by current T-Mobile towers, this number was subtracted by 100. 41.1% of LA County is covered by T-Mobile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-added-towers.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above map offers an insight to what 3 additional cell towers would achieve for improving T-Mobile’s cell service in LA County. The placement of added towers was chosen in areas lacking in coverage: Malibu, Palmdale, and Lancaster. The same parameters for calculating coverage of this scenario were held constant with the prior map. The addition of 3 towers had the most significant result on increasing T-Mobile total coverage (a total of 45.8%).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/25k-to-30k.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map represents the impact of an increased radius of 5,000km — from 25,000km to 30,000km. While one would assume increasing the radius by a number as staggering as 5,000km would alter coverage significantly, a mere of 0.6% difference was achieved. The same methodology for total coverage area was applied: dividing the maximum (this time 7,172,600,041) by the sum (12,301,062,346) and subtracting by 100. 58.3% was the result, and when subtracted by 100 left for a total coverage of 41.7% in LA County.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/increased-towers-10m.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This final map considers the difference an alteration of 10 meters in cell tower height might have on T-Mobile coverage in LA County. Interestingly, altering the tower height (as opposed to increasing the range) had a much more significant impact on cell coverage area. By increasing the towers’ height by 10 meters, the coverage area increased by almost 2% (a total of 43.6%). The same viewshed analysis factors were held constant: dividing the max (this time 6,935,051,785) by the sum (12,301,062,346), to get the total area not covered (56.4%). This number was then subtracted by 100 to get the total area covered.&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Aug 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2017/08/29/T-mobile-cell/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/29/T-mobile-cell/</guid>
        
        <category>Raster Analysis</category>
        
        
      </item>
    
      <item>
        <title>A New Design for Sequoia National Park: The Ideal Cartographic Relief Map</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/Sequoia-National-Park.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 06 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2016/09/06/sequia-national-park/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/09/06/sequia-national-park/</guid>
        
        <category>Cartography</category>
        
        
      </item>
    
      <item>
        <title>Re-Designing LA's Bus Routes: Big Blue Bus #44</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/Bus-Route-1.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This generalized map of Santa Monica’s ‘Big Blue Bus’ transit route #44 was created through two basic functions: Arctoolbox’s draw function and the employment of point/line symbology. The points (bus stops) were obtained online; a query builder was performed to isolate the required stops/destinations along route #44. In total, there were 11 bus stops in-between the origin and destination points. These points were given a bold circular symbology to denote each stop. In addition, ‘callouts’ were created with the draw feature to label the bus stops in a way that was physically pleasing to the viewer’s eye, but doesn’t obstract the bus route. The same process was employed to create the physical bus route between each stop location — the route data was obtained online, and then a query builder was performed to generalize bus #44’s route in its entirety. The use of an ESRI Basemap was essential for deducing the accuracy behind this generalized map; the Basemap allowed for a visualization of acute discretions between each bus stop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Bus-Route-2.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This second generalized map is one less reliant on the view of a Basemap: a map that relies on the accuracy of various accentuated locations throughout Santa Monica. However, the creation of this customized map was achieved through toggling on-and-off of an ESRI Basemap to allow for the most minute details. Key features drawn on the map (Santa Monica Freeway, Stewart Street Park, Memorial Park, John Adams Middle School, and Clover Park), were carefully traced over a Basemap - like georeferencing but without the need of a jpg image. The symbology for bus locations and Bus Route #44 were not altered in this second map. This map relied much more heavily on the draw functionality than the previous map, as the created structures listed above essentially symbolize the viewer’s sense of awareness in Santa Monica. The Santa Monica Freeway is represented by bold yellow and dashed lines, because (as always).. Go Bruins!!&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2016/09/04/Bus-Route/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/09/04/Bus-Route/</guid>
        
        <category>Cartography</category>
        
        
      </item>
    
      <item>
        <title>Recreating the 2016 Presidential Election: A Nation Divided</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/2016-Election.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2016/09/03/Presidential-Election/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/09/03/Presidential-Election/</guid>
        
        <category>Cartography</category>
        
        
      </item>
    
      <item>
        <title>Variables of Statewide Thematic Mappings</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/ed-attainment.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/employment-rate-change.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pct-rural.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/regional-mapping.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/total-pop-per-state.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2016/09/02/statewide-thematic-mappings/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/09/02/statewide-thematic-mappings/</guid>
        
        <category>Cartography</category>
        
        
      </item>
    
      <item>
        <title>A Dot-Density Representation of LA's Ethnic &amp; Racial Distribution</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/images/Racial-Dot-Density-LA.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2016/09/01/Dot-Density/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/09/01/Dot-Density/</guid>
        
        <category>Cartography</category>
        
        
      </item>
    
      <item>
        <title>Three-Dimensional Mapping of Natural Disaster Scenarios: Leilani Estates, HI (2018)</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;The Hawaii islands were construed by volcanic activity, both past and present. In 2018, lava began flowing from steaming ground cracks in Leilani Estates on the Big Island. The maps produced below showcase this volcanic crisis - a tranquil place of palm trees transformed into hostility and hot rock. The three month-long eruption of the Kilauea Volcano formed 24 volcanic fissure vents, destroyed 200 homes, and wreaked havoc on communities that forced the diaspora of roughly 2,000 people.&lt;/p&gt;

&lt;p&gt;The static map below illustrates the buildings at harm in Leilani Estates, along with the 24 volcanic fissures created from the Kilauea eruption. Areas previously affected by volcanic activity are also highlighted.&lt;/p&gt;

&lt;p&gt;A three-dimensional rendered mapping of the event was generated through a QGIS plugin called ‘qgis2web.’ An interactive map helps reinforce the severity of a natural disaster scenario like that of Leilani Estates in 2018.&lt;/p&gt;

&lt;h3 id=&quot;leilani-estates-hi---recent-volcanic-activity&quot;&gt;Leilani Estates, HI - Recent Volcanic Activity&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Leilani-Estates.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3d-rendering-of-volcanic-fissures&quot;&gt;3D Rendering of Volcanic Fissures&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3d-leilani.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 09 Sep 2015 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2015/09/09/Leilani-Estates/</link>
        <guid isPermaLink="true">http://localhost:4000/2015/09/09/Leilani-Estates/</guid>
        
        <category>DEM</category>
        
        
      </item>
    
      <item>
        <title>Bridging the Darién Gap: A Cost-Distance Analysis of Yaviza to Chigorodo and Apartado</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This study illustrates a cost-distance analysis potential to calculate the shortest (and most feasible cost affordability) from destination A to destination B. Or, in this case, an origin (Yaviza) to two countering destinations (Chigorodo and Apartado), also known as the Darién Gap in Colombia. 3 main variables were weighted when predicting the shortest path from Yaviza: total slope, amount of water present, and the flow accumulation of the present water in South America. Because these variables represent varying weights in influence — slope being weighted the highest in regard to the elevation gradient in South America — each constant was held at a different weight when calculating the path’s trajectory.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;The first map embodies the summation of this entire process: an illustration of the least cost path from an origin point (Yaviza) to two counter destination points (Chigorodo and Apartado), with consideration towards elevation terrain slope, bodies of water, and the flow accumulation of that water. The country’s border between Panama, as well as the most important road networks, are further accentuated in the map. A conglomerate of Digital Elevation Maps (4) were downloaded for the state of Colombia, and then combined into a single TIFF. A slope calculation was then run on the combined TIFF. To calculate the presence/influence of water in Colombia, a raster calculation was performed with a score of less than or equal to 0; this ultimately highlighted what was water (a score of 1) and what wasn’t water (a score of 0). The slope was then reclassified between two values — ‘Old’ (with a range of 0 to 60), and ‘New’ (with a range of 1 to 512). It is as follows below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/darien-reclass.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With this reclassification, a raster calculation was again undergone. This time, considerations for both the presence of water and the terrain slope elevation were made. The reclassified terrain slope was added to the water calculation and multiplied by a value of 128. It was decided that the flow accumulation of water was another significant value to be analyzed when deciding the least cost-effective path between the origin and two destinations. The ‘Spatial Fill’ button was used to find the amount of accumulated water. The direction of water was realized, and further used to calculate the total accumulation of water in Colombia. The direction of water was divided into two classes - the first being of values between 0 and 536,711, and the second being values between 536,711 and 1,631,109. The final value was found using the following raster calculation: &lt;strong&gt;Con(“Flow_Accumulation” &amp;gt;= 536,711, 1, 0)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The origin and destination points were found using an online data source (click2shp). This allowed for the cost distance between points to be made. Calculating this cost distance made finding the quickest cost path from Yaviza to Chigorodo and Apartado possible - the final product shown in both maps. This process was done for Chigorodo and Apartado separately, and then combined into a single layer. The elevation gradient for the road paths between the two destination locations was achieved through the ‘Stack Profile’ operation in the 3D Analyst toolbox. Regions with higher slopes, a more noticeable presence of water, and greater flow accumulation, are more expensive to build roads than areas where these variables are of lower values.&lt;/p&gt;

&lt;h3 id=&quot;least-cost-path-analysis-crossing-the-darién-gap&quot;&gt;Least-Cost Path Analysis: Crossing the Darién Gap&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/darien-least-cost.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The map above illustrates the most affordable method of building a road between Yaviza to Chigorodo and Apartado: where the slope is least, the presence of water is bare to minimum, and the total flow accumulation of water is marginable.&lt;/p&gt;

&lt;h3 id=&quot;3d-rendering-of-least-cost-path&quot;&gt;3D Rendering of Least-Cost Path&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3d-darien.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ArcScene was used for creating this map - a 2D visualization of the least cost path between the points (the static, reference map). Base Heights were altered to emphasize the presence of mountains (slope) in Colombia, as well as the bodies of water and the least-cost path. The total elevation was changed to a value of 3, and the layers were offset to a degree of 50. Lighting was heightened through the ‘3D Effects’ feature, accentuating the presence of our variables of interest.&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Sep 2015 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2015/09/07/Darien-Gap/</link>
        <guid isPermaLink="true">http://localhost:4000/2015/09/07/Darien-Gap/</guid>
        
        <category>DEM</category>
        
        
      </item>
    
      <item>
        <title>Potential Inundation: Sea Level Rise in Miami-Dade County, Florida</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This study explores areas of potential inundation through the analysis of a high-resolution digital elevation model (DEM), and performance of sea level change scenarios of varying severity (Moderate, Severe, and Extreme). The areas of potential inundation are visually superimposed such that the most vulnerable areas appear darker. The application of Boolean expressions using raster calculations allows for the accentuation of areas currently above and below sea level in Miami-Dade County, Florida.&lt;/p&gt;

&lt;h3 id=&quot;mapping-sea-level-rise---a-dem-of-miami-dade-county&quot;&gt;Mapping Sea Level Rise - A DEM of Miami-Dade County&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Inundation.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Sep 2015 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2015/09/06/Florida-Inundation-DEM/</link>
        <guid isPermaLink="true">http://localhost:4000/2015/09/06/Florida-Inundation-DEM/</guid>
        
        <category>DEM</category>
        
        
      </item>
    
      <item>
        <title>Comprehending the Range of North Korea's Strongest Missiles</title>
        <description>&lt;h3 id=&quot;intro&quot;&gt;Intro:&lt;/h3&gt;

&lt;p&gt;Below are 3 maps that illustrate various ranges of North Korea’s longest ranging missiles: The Hwasong-14 (1,553 miles), the Intermediate Ballistic Missile (2,299 miles), and the Hwasong-16 (8,000 miles). Each missile’s range is denoted by a buffer, which is ascribed the distance of the missile’s projected distance of impact. Additionally, each map showcases which country (highlighted in red) is within range of North Korea’s missiles. Major cities — ranked by having a population of 5 million and higher — are further displayed on each map.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;The range of North Korea’s 3 strongest missiles were obtained online. A world country shapefile was provided through the UCLA GIS Geoportal. North Korea (highlighted in dark red) was isolated by a query builder on the world county shapefile, and then exported as a new layer. Three buffers representing each the missiles’ extent were created around the North Korea layer. Each buffer was converted from decimal degrees to miles. To isolate the countries that intersect within the range of these buffers, a select by location tool was used. A world city shapefile was then downloaded from the UCLA GIS Geoportal. To isolate the major cities, a query builder was performed to isolate the cities labeled as ‘Rank 1’ (a population of 5 million or higher). These countries are labeled in blue.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;The Varying Impact of North Korea’s 3 Strongest Missiles&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Hwasong-14.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map displays the range of North Korea’s Hwasong-14 missile, which can reach a distance of 1,553 miles. The countries at risk within this radius are Russia, Mongolia, China, South Korea, Japan, Vietnam, the Northern Mariana Islands, and the Philippines. The Major cities within this range are Beijing, Seoul, Tokyo, Taipei, Hong Kong, and Manila (labeled in blue).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Intermediate-Ballistic.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map illustrates the range of North Korea’s Intermediate Ballistic missile, which can cover a distance of 2,299 miles. With an expanded radius of 746 miles, there are 14 additional countries at risk: Kazakhstan, Kyrgyzstan, Nepal, India, Bhutan, Bangladesh, Myanmar, Laos, Thailand, Vietnam, Cambodia, Malaysia, Brunei, Palau, Guam, and parts of Micronesia. Moreover, 4 new major cities fall within the Intermediate Ballistic missile’s range: Lahore, Delhi, Mumbai, Bangalore, and Bangkok.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Hwasong-16.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above map showcases the range of North Korea’s Hwasong-16 Missile, which extends a distance of 8,000 miles in orbit. If launched, the Hwasong-16 would reach every continent but South America. It is North Korea’s longest tested missile. The new major cities at risk would entail Moscow, Tehran, Karachi, Baghdad, Cairo, Istanbul, London, Lagos, Kinshasa, New York and Mexico City. The only major cities not at-risk are in South America, namely Bogota, Lima, Rio de Janeiro, Sao Paulo and Buenos Aires.&lt;/p&gt;
</description>
        <pubDate>Sat, 13 Sep 2014 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2014/09/13/North-Korea/</link>
        <guid isPermaLink="true">http://localhost:4000/2014/09/13/North-Korea/</guid>
        
        <category>SQL &amp; Buffer Analysis</category>
        
        
      </item>
    
      <item>
        <title>Starbucks' Key to Success: Demographic Considerations for Ideal Store Locations in Riverside County, CA</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This study centers around a network analysis method of market-based locations - the locations of Starbucks coffee shops in Riverside, County. Data for the store locations was obtained through OpenStreetMap. Network analysis tools were used to generate 3-minute drive time buffers between each coffee shop (Map 1). Map 2 illustrates the full use of Network Analysis Tools: to generate distance-based buffers (total drive time) between each Starbucks store, with a store recommended for closure as the origin (the Target Arlington Starbucks).&lt;/p&gt;

&lt;p&gt;The remaining 3 maps showcase the drive time buffer joined to census block data in Riverside County, allowing for an analysis of the summation of blocks that intersect within these stores with 3 main variables: Total population density (Map 3), a targeted age demographic between the ages of 25-34 (Map 4), and the blocks’ median household income (Map 5).&lt;/p&gt;

&lt;p&gt;Improvements could be made to Map 2 by making the ‘Drive-Time’ more readable; less decimals of rounded estimates and the inclusion of ‘Minutes’ would allow the viewer to easily understand the time it takes to drive to the proposed elimination of the Target Arlington Starbucks.&lt;/p&gt;

&lt;h3 id=&quot;the-importance-of-drive-time-starbucks-store-locations-in-riverside-county-ca&quot;&gt;The Importance of Drive-Time: Starbucks Store Locations in Riverside County, CA&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-Minute-Drive-Time-Buffers.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This static map illustrates a 3-minute drive time service area between each Starbucks coffee shop. However, one store was purposely covered by the legend in each map (Vons Starbucks), due to its outside placement on the border of Riverside near San Bernardino. In full, a little over 70% of Riverside’s population (472,087 people out of 593,012) fall within a 3-minute radius (drive time) of any given Starbucks coffee shop. Moreover, the highways in Riverside are depicted in red, a bright observation that allows for an interesting parallel to be made: Starbucks coffee shops are generally found off the highways, most likely as pit-stops for drivers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Eliminated-store-based-on-drive-time.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map depicts a grand use of network analysis tools: To produce buffers that correspond to an estimation of distance traveled between each Starbucks location, generated by using the proposed store for closure as the origin. These buffers (represented as black lines) correspond to the total drive time between the stores. The store with the longest drive time is the store excluded from most maps (nearly a 14-minute drive time). The largest reason for closing the Target Arlington Starbucks is its proximity to 7 other store locations, each being a less than 5-minute drive time from the store.&lt;/p&gt;

&lt;h3 id=&quot;understanding-your-targeted-audience-starbucks-demographic-analysis&quot;&gt;Understanding Your Targeted Audience: Starbucks’ Demographic Analysis&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Age-Demographic.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The map above illustrates a nuanced range of age groups (years 25 to 34 in age), used as Starbucks’ primary consumers, along with the 3-minute drive time, store location, and the background highways of Riverside. As one would expect, the larger the concentration of individuals between this age range parallels the higher concentration of Starbucks coffee shops. In specific, blocks with a range of 250 to 900 residents have the closest proximity to a Starbucks location — the mogul’s attempt to build closer to their targeted audience.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Median-Income.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map denotes the median household income for the residents of Riverside County, coupled with the same 3-minute drive time, the Starbucks coffee shops locations, and the outlined highways. The range is between a low of $36,042 and a high of $188,077. Although the map shows a somewhat scattered representation of the median income block data, it can be roughly stated that the Southern region of Riverside County lies within the range of $70,000 and higher, while the central and Northern region of Riverside is between $36,000 to $70,000. The service area for each given Starbucks (joined with the census block data for total population size), allows for an interesting analysis of store placement: the largest bulk of Starbucks’ buyers fall within an income of less than roughly $75,000. This map mirrors the age group generalization — blocks with larger quantities of individuals aged 25-39 are also the blocks with higher incomes and are likewise closer to a Starbucks coffee shop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Pop-Density.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The map above is used as a general representation of understanding the Starbucks store locations desire placement, corresponding to Riverside’s total population. I decided to use the total population as the broad basis for this analysis to conjure a general and easily predicted understanding. This deviates from the earlier maps, which focused on a more nuanced representation that allowed for a more targeted analysis. The range is between 1,199 residents (the lightest shading) per block, and 7,700 per block (the darkest shading). Roughly 8 out of 11 Starbucks locations fall within a block of 2,300 residents or higher — a vast understanding to Starbucks trend of store placement. Blocks with residents of 2,300-7,700 hold the majority of Starbucks audience buyers. Starbucks conveniently placed store locations off the highways to make up for the remaining blocks with less people, ergo the blocks with less possible buyers.&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Sep 2014 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2014/09/12/Starbucks/</link>
        <guid isPermaLink="true">http://localhost:4000/2014/09/12/Starbucks/</guid>
        
        <category>SQL &amp; Buffer Analysis</category>
        
        
      </item>
    
      <item>
        <title>Elementary School Attendance &amp; Number of Registered Voters Per School District: Minimizing Classroom Sizes Clark County, NV</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;The premise of this analysis is to illustrate the relationship between elementary school attendance and number of registered voters in Clark County, NV (District C). This school district holds the highest number of students in the county, making complaints of cramped classrooms and lack of available transportation provided by the school a common occurrence.&lt;/p&gt;

&lt;p&gt;The idea behind this spatial overlay and geoprocessing process is to create maps that indicate a need for school attendance areas (elementary schools within county-level boundaries) to minimize their class sizes. It also reinforces student safety protocols - the underlying base map accentuates available bus routes, or lack thereof for many students who live more than one-mile from the nearest school.&lt;/p&gt;

&lt;h3 id=&quot;the-correlation-between-elementary-school-attendance--registered-voters-clark-county-nv&quot;&gt;The Correlation between Elementary School Attendance &amp;amp; Registered Voters: Clark County, NV&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/school-attendance.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map represents the number of elementary schools in ‘District C’ of Clark County, Nv - the most populated school district in the county. It also illustrates the borders of Attendance Areas, and the locations of registered voters (homes).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/registered-votes-per-school.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The choropleth map above showcases Attendance Areas with a breakdown by number of voters in District C. The values for registered voters per school were obtained through a field calculation of attendance zones. The names of the schools associated with the attendance areas are also included.&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Sep 2014 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2014/09/11/school-attendance-clark-county/</link>
        <guid isPermaLink="true">http://localhost:4000/2014/09/11/school-attendance-clark-county/</guid>
        
        <category>SQL &amp; Buffer Analysis</category>
        
        
      </item>
    
      <item>
        <title>Public School and Oil Gas Well Assessment in Los Angeles, CA</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;The curation of these maps focalized on the utilization of Geoprocessing techniques (namely Buffers), and application of ‘Select by Location’ tools within QGIS. When uploading the data points - the locations of oil gas wells and public schools in LA County - obtained from various sites, buffers were created of varying lengths around each data point. The buffers were assigned 3 different parameters: 1,000 feet, a half-mile, and one-mile. These 3 varying buffer segmentations were applied to both public school locations and oil gas well locations. To find which public schools fell within an oil gas well radius of fluctuating lengths (1,000 feet, half-mile, and one-mile), the ‘Select by Location’ tool was used; schools that intersected within the varying buffers were highlighted, and extracted as independent shapefiles. This same process was used for oil gas wells, but in reverse — the ‘Select by Location’ tool highlighted the oil gas wells that intersected with the varying buffers created around each public school in Los Angeles County.&lt;/p&gt;

&lt;p&gt;The choropleth map included illustrates the spatial correlation of Median-Household Income in Los Angeles County, in an attempt to accentuate the placement of public schools that fall within 1-mile of oil gas wells that exhibit hydraulic Fracking. This process was completed through a simple join, with the Los Angeles County tracts acquired from the US Census Bureau, and the tract metadata provided through the UCLA Geoportal. The result was a map portraying the variability of household income, and the association of lower-income tracts holding a higher percentage/likelihood of having a public school within a one-mile radius of oil gas wells that exhibit hydraulic fracking.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Analysis of Proximity Based Hazards:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;21,259 oil gas wells are spread across the breadth of Los Angeles County. Of these, 1,664 fall within 1,000 feet of a given public school; 8,091 fall within a half-mile, and 16,067 (over 75%) fall within a one-mile radius of any given public school.&lt;/li&gt;
  &lt;li&gt;There are 1,974 public schools within the jurisdiction of Los Angeles County. Of these, 256 fall within 1,000 feet of an oil gas well; 730 fall within a half-mile, and 756 (almost 40%) fall within one-mile of any given oil gas well.&lt;/li&gt;
  &lt;li&gt;There are over 10 times as many oil gas wells as there are public schools in Los Angeles County.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;oil-gas-well-distribution-in-relation-to-public-school-locations-los-angeles-ca&quot;&gt;Oil Gas Well Distribution in Relation to Public School Locations: Los Angeles, CA&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-1.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-2.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-3.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-4.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-5.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-6.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-7.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-8.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-9.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/map-10.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion:&lt;/h3&gt;
&lt;p&gt;Because of the massive quantity of oil gas wells in LA County, it makes sense that there would be an overlap in place location — many oil gas wells being placed less than one-mile from one another. However, the severity of having so many public schools fall within this extent has grave implications. The most extensive hazard would be the likelihood of a gas leak, allowing harmful toxins to cause life-long health problems for Los Angeles County’s youth population. Deadly chemicals can quickly spread these short distances from public schools to oil gas well locations and may not be initially caught by the public eye. This, coupled with the fact that emission rates in oil gas wells are intensified, will harm the adolescents who are closest to the site (256 schools within such a short proximity as 1,000 feet).&lt;/p&gt;

</description>
        <pubDate>Wed, 10 Sep 2014 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2014/09/10/Public-School-Oil-Gas-Well/</link>
        <guid isPermaLink="true">http://localhost:4000/2014/09/10/Public-School-Oil-Gas-Well/</guid>
        
        <category>SQL &amp; Buffer Analysis</category>
        
        
      </item>
    
      <item>
        <title>Conceptualizing the Shift from Conventional Vehicles to Electric Vehicles: Modeling a Proposed 'Feebate Program'</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This model manifests a palpable representation of a conceptual Feebate Program that advocates for the reduction of conventional vehicles, and accentuation of electric vehicles. Essentially, the model illustrates the effects of altering sales in conventional vehicles and electric vehicles in the state of California - a shift that underscores the dramatic impact each vehicle has on the atmosphere’s global carbon content. In this model, the juxtaposing roles of rebates and fees are intertwined into a single program (a feebate). The Feebate Program symbolizes the contrasting binaries at work here: Both rebates offered for electric vehicles and fees imposed on conventional vehicles emblemize the incentives for purchasing electric vehicles, by increasing the total amount of electric vehicles being sold.&lt;/p&gt;

&lt;p&gt;This purchase-price program is assumed to be the largest force impacting consumer decisions; the larger the rebate offered per electric vehicle, and the larger the fee imposed per conventional vehicle, the higher the rate of electric vehicles being purchased. However, the issue with using an economic model to explore the impacts on an atmospheric scenario, stems from the flow of cash in the economy (the ‘Total Fund Balance’ in the model). The cash flow is constantly fluctuating and can neither be too large to strip the economy of too much money, nor too insignificant to hinder the optimum flow within the economy itself.&lt;/p&gt;

&lt;p&gt;Although this model is economic in nature, it emphasizes the environmental impact of consumer purchases — electric vehicles exude drastically lower rates of carbon dioxide than conventional vehicles. Ultimately, this model will achieve an overall reduction in total atmospheric carbon dioxide vis-à-vis the shift in consumer purchases from conventional vehicles to electric vehicles.&lt;/p&gt;

&lt;h3 id=&quot;description-of-feebate-model&quot;&gt;Description of Feebate Model:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/feebate-model.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stocks:&lt;/strong&gt;
This model is comprised of three stocks: ‘Total Fund Balance,’ ‘Number of Conventional Vehicles,’ and ‘Number of Electric Vehicles.’ The initial value for the ‘Total Fund Balance’ stock was set to $100,000,000 dollars - an amount large enough to simulate the conceptual Feebate Program of absorbing fees and distributing rebates. This was the only stock set to have a finite number—both the ‘Number of Conventional Vehicles’ stock and the ‘Number of Electric Vehicles’ stock, representative of the number of the cars in production between the two types, were set to zero. This initial value of zero allows for the total effectiveness of the feebate program to rise to the surface, creating a sense of scale in the impact between conventional vehicles and electric vehicles on the environment.&lt;/p&gt;

&lt;p&gt;Data provided by online sources indicated the average rebate offered for an electric car is around $7,000 dollars - and the average number of electric cars in production in the state of California is roughly 142,856.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Flows:&lt;/strong&gt;
There are 6 flows in the Feebate Model. The ‘Total Fund Balance’ stock is comprised of two flows: ‘Fees Collected’ (the source of the stock’s monetary collection), and ‘Rebates Paid’ (the source of the stock’s monetary loss). The ‘Fees Collected’ input flow is summed by the influence of three variables: The ‘Sales of Conventional Vehicles,’ ‘Fees Imposed per Conventional Vehicle,’ and a hypothetical ‘Added Late Fee,’ (discussed below, used when testing Pulse Disturbance).&lt;/p&gt;

&lt;p&gt;The total ‘Fees Collected’ flow is calculated by multiplying the variables ‘Sales of Conventional Vehicles’ and ‘Fees Imposed per Conventional Vehicle,’ with the addition of a possible ‘Added Late Fee’ variable. The output flow for the ‘Total Fund Balance’ stock, ‘Rebates Paid,’ is calculated through the multiplication of two variables - ‘Rebate Given per Electric Vehicle,’ and ‘Sales of Electric Vehicles’ (discussed below). The ‘Number of Conventional Vehicles’ stock is likewise comprised of two flows: The ‘Sales of Conventional Vehicles,’ and the number of ‘Retired Conventional Vehicles.’&lt;/p&gt;

&lt;p&gt;The input flow ‘Sales of Conventional Vehicles’ is dependent on a single converter: ‘Buyer Response to Conventional Vehicle Fees’ (discussed below). The output flow ‘Retired Conventional Vehicles’ is derived from stock itself, the ‘Number of Conventional Vehicles,’ divided by the ‘Lifetime of Conventional Vehicles’ variable (discussed below).&lt;/p&gt;

&lt;p&gt;The remaining two flows in the model, ‘Sales of Electric Vehicles’ and ‘Retired Electric Vehicles,’ comprise the ‘Number of Electric Vehicles’ stock. The flows of this stock mirror the flows of its counterpart, conventional vehicles. The input flow is derived from a single converter: ‘Buyer Response to Electric Vehicle Rebates.’ Similarly, the output flow of the number of ‘Retired Electric Vehicles’ is calculated by dividing the ‘Number of Electric Vehicles’ stock by the ‘Lifetime of Electric Vehicles’ variable (discussed below).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variables:&lt;/strong&gt;
Seven variables influence the oscillations witnessed throughout the model, altering the dynamics of the flows and stocks discussed above. Each flow has a minimum of one variable influencing its total input or output. For instance, the ‘Fees Collected’ flow is dually impacted by the ‘Added Late Fee’ variable, and the ‘Fees Imposed Per Conventional Vehicle’ variable. During the initial run of the model, ‘Fees Imposed Per Conventional Vehicle’ was set to zero dollars. By setting the variable to an initial state of zero, essentially having no impact on the system whatsoever, the facet of carbon dioxide in the system was fully emphasized — without the influence of an implemented fee for the purchase of a conventional vehicle, the sale of conventional vehicles would continue to stay constant or rise.&lt;/p&gt;

&lt;p&gt;The ‘Fees Imposed Per Conventional Vehicle’ variable is meant to directly influence the buyer’s decision to purchase a conventional vehicle or not. This variable was set to be a slider that allows for fluidity between multiple scenarios, denoting the effect of a zero-dollar fee per conventional vehicle, compared to that of a $3,000 dollar fee per vehicle, for instance.&lt;/p&gt;

&lt;p&gt;The ‘Rebate Given Per Electric Vehicle’ variable plays the same role as its counterpart; it directly influences the buyer’s decision to purchase an electric vehicle or not. If the rebate offered is too small to influence the total cost of the electric car, which is more expensive than a conventional vehicle, the buyer will most likely turn to conventional vehicles.&lt;/p&gt;

&lt;p&gt;The ‘Rebate Given Per Electric Vehicle’ variable was likewise set to be a slider, allowing for a multi-faceted influence through various scenarios of rebates offered. The initial rebate was ascribed $7,000 dollars, with a max rebate offered at $10,000 dollars. This value was chosen based on the average value given for rebates for ‘clear air vehicles’ in the state of California. According to the California Clean Vehicle Rebate Project administered by CSE for the California Air Resources Board, “California residents get up to $7,000 for the purchase or lease of a new, eligible zero-emission or plug-in hybrid light-duty vehicle.”&lt;/p&gt;

&lt;p&gt;The variables ‘Lifetime of Conventional Vehicle’ and ‘Lifetime of Electric Vehicle,’ refer to the battery life of each vehicle. The average battery life of a conventional vehicle is four years, while the average battery life of an electric vehicle is roughly 12 years. Based on this observation, conventional vehicles have quicker retirement rates than electric vehicles: the longer the battery life of a vehicle, the longer a vehicle stays in production. This assumption does not include the percentage of individuals who replace car batteries.&lt;/p&gt;

&lt;p&gt;The remaining three variables are at the core of this model’s discovery: the influence of conventional vehicles and electric vehicles on the total carbon dioxide content in the atmosphere. The ‘Amount of CO2 Emitted Per Conventional Vehicle’ variable is set at 5.19 tonnes of CO2. This amount is more than five times that of the ‘Amount of CO2 Emitted Per Electric Vehicle,’ a trivial 0.98 tonnes of CO2.&lt;/p&gt;

&lt;p&gt;These two variables, along with the ‘Number of Conventional Vehicles’ stock and ‘Number of Electric Vehicles’ stock, determine the final variable: ‘Total Carbon Dioxide Content.’ The ‘Total Carbon Dioxide’ variable was estimated through the following formula: &lt;strong&gt;Total Carbon Dioxide = ([Number of CV] * [Amount of CO2 Emitted Per CV]) + ([Number of EV] * [Amount of CO2 Emitted Per EV]).&lt;/strong&gt; This variable records the amount of carbon dioxide produced by both conventional and electrical vehicles.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Converters:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The model’s two converters, ‘Buyer Response to Conventional Vehicle Fees’ and ‘Buyer Response to Electric Vehicle Rebates,’ are symbolic of the force driving the entire model: The Feebate Program. The higher the fee imposed per conventional vehicle, the more likely the buyer response will to shift towards the purchase of electric vehicles. Likewise, the higher the rebate offered for electric vehicles, the more likely the buyer response will shift away from conventional vehicles and towards electric vehicles. The buyers’ response is the key to reducing the total amount of carbon dioxide in the model.&lt;/p&gt;

&lt;h3 id=&quot;confidence-building--sensitivity-analysis-testing&quot;&gt;Confidence Building &amp;amp; Sensitivity Analysis Testing:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Confidence Building:&lt;/strong&gt;
Optimization was incorporated into the variables ‘Fees Imposed Per Conventional Vehicle,’ and ‘Rebate Given Per Electric Vehicle.’ Using these two variables, the model was optimized in order to minimize the ‘Total Carbon Dioxide’ variable. The minimum bound for the variable ‘Fees Imposed Per Conventional Vehicle’ was set to zero dollars; the maximum bound was set to $6,500 dollars. These minimum and maximum bounds come from the minimum and maximum values that were set to the slider for the variable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CO2-Optimization-results.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, the minimum value for ‘Rebate Given Per Electric Vehicle’ variable of $7,000 dollars, and the maximum value of $10,000 dollars, stems from the minimum and maximum possible values set for the slider of the variable. The results from this optimization illustrate that the optimal minimum amount of tonnes of carbon dioxide will be produced if the ‘Fees Imposed Per Conventional Vehicle’ variable is set to $3,250 dollars, and ‘Rebate Given Per Electric Vehicle’ is set to $8,500 dollars.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/C02-Optimization-graph.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monte-Carlo Sensitivity Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The sensitivity of this model was tested on the premise of four variables: ‘Lifetime of Elective Vehicles,’ ‘Lifetime of Conventional Vehicles,’ ‘Amount of CO2 Emitted per Electric Vehicle,’ and ‘Amount of CO2 Emitted per Conventional Vehicle.’ The fix function was used in all four of the Monte-Carlo sensitivity analysis tests. The employment of the fix function was to provide a simulation without randomness over time. The output of the model is fully determined by the parameters and initial conditions. Although the values used for the variables ‘Lifetime of Electric Vehicle,’ ‘Lifetime of Conventional Vehicle,’ ‘Amount of CO2 Emitted per Electric Vehicle,’ and ‘Amount of CO2 Emitted per Conventional Vehicle,’ have slightly differing values for the given parameter, it is with certainty that these values do not change over time.&lt;/p&gt;

&lt;p&gt;Realistic values for each of the variables were chosen in order to see how sensitive the model is too small, yet still plausibly effective, changes in the input value for the variable. Using the fix function, the parameter value changes randomly between simulations — and each simulation that is ran is a deterministic solution. For the variable ‘Lifetime of Electric Vehicle,’ the equation of &lt;strong&gt;Fix(Rand(10,12))&lt;/strong&gt; was used to create a Monte-Carlo sensitivity analysis test. The values of 10 and 12 were chosen for the minimum and maximum range, in an attempt to observe the model’s true sensitivity to other potentially valid values for the variables.&lt;/p&gt;

&lt;p&gt;The results below illustrate that changing the value for the variable ‘Lifetime of Electric Vehicles’ does not have a large effect on the total amount of carbon dioxide produced, measured by the variable ‘Total Carbon Dioxide.’&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/EV-Variable-Equation.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/EV-Monte-Carlo.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the ‘Lifetime of Conventional Vehicles’ variable, a Monte-Carlo sensitivity analysis test was created using the equation &lt;strong&gt;Fix(Rand(3,5)).&lt;/strong&gt; A value of 3 was chosen for the minimum and the value of 5 was chosen for the maximum. These minimum and maximum values were determined by the knowledge provided by the American Automobile Association, which expressed an average short battery lifetime of conventional cars of 3 years, and average long battery lifetime of conventional cars at 5 years.&lt;/p&gt;

&lt;p&gt;The results below illustrate that changing the value for the variable ‘Lifetime of Conventional Vehicles’ causes a larger variation in the total amount of carbon dioxide, much more than changing the value for the variable ‘Lifetime of Electric Vehicles.’&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CV-Variable-Equation.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CV-Monte-Carlo.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the ‘Amount of CO2 Emitted per Electric Vehicle’ variable, the fix function was once again used. The equation &lt;strong&gt;Fix(Rand(0,2))&lt;/strong&gt; was employed to simulate how sensitive the model is to changes in the amount of the variable itself (CO2). The minimum value was set to 0 - it’s possible no carbon dioxide may be emitted from the electric vehicle. The maximum value was set to 2, assuming a small portion of carbon dioxide may be emitted over the course of the vehicle’s lifetime. Similar to the sensitivity analysis for the variable ‘Lifetime of Electric Vehicles,’ changing the values for the amount of carbon dioxide emitted per electric vehicle does not have a large effect of the total amount of carbon dioxide produced in the atmosphere.&lt;/p&gt;

&lt;p&gt;The results below represent the changes in carbon dioxide emitted per electric vehicle, based on the parameters described above.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/EV-CO2-Emitted.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/EV-sensitivity-graph.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final sensitivity analysis test was conducted for the ‘Carbon Dioxide Emitted per Conventional Vehicle’ variable. For this simulation, the equation &lt;strong&gt;Fix(Rand(5,10))&lt;/strong&gt; was used. This minimum value was estimated based on the maximum value ascribed for the variable ‘Carbon Dioxide Emitted per Electric Vehicle,’ holding true that conventional vehicles emit more carbon dioxide than their counterpart. The maximum value of 10 was selected to test sensitivity for the model, taking into account the notion of older model cars producing more carbon dioxide than newer model cars.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CV-C02-Emitted.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Total-CO2-Sensitivity.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar to the to the sensitivity analysis for the variable ‘Lifetime of Conventional Vehicles,’ changing the values for the variable ‘Carbon Dioxide Emitted per Conventional Vehicle’ creates a large variation in the effect of the total amount of carbon dioxide produced. Through this discovery, it is determined the Feebate Model is most sensitive to changing the values for the variables related to conventional vehicles - ‘Lifetime of Conventional Vehicle’ and ‘Carbon Dioxide Emitted per Conventional Vehicle - rather than the variables related to electric vehicles - ‘Lifetime of Electric Vehicle’ and ‘Carbon Dioxide Emitted per Electric Vehicle.’&lt;/p&gt;

&lt;h3 id=&quot;feebate-model-incites-policy-change&quot;&gt;Feebate Model Incites Policy Change:&lt;/h3&gt;

&lt;p&gt;Through the implementation of sliders in the Feebate Model on the variables ‘Fees Imposed Per Conventional Vehicle’ and ‘Rebates Given Per Electric Vehicle,’ a proposed bill can be introduced to enact policy change. In theory, a government bill emblemizing this Feebate Model would drastically alter the amount of carbon dioxide emitted into the atmosphere every year. This bill would dictate the amount of fees imposed when purchasing conventional vehicles, and the amount of rebates given when purchasing electric vehicles.&lt;/p&gt;

&lt;p&gt;The heart of this policy change is demonstrated in the palpable results derived from the Sensitivity Testing and Optimization graphs. Increasing rebate amounts given for electric vehicles from $7,000 dollars (the base model) to $8,500 dollars (optimization model), coupled with an increased fee for conventional vehicles from zero dollars (current) to $3,250 dollars (optimization model), would drastically reduce the amount of carbon dioxide produced from 340,000,000 Tonnes to 25,000,000 Tonnes. A 92.65% decrease.&lt;/p&gt;

</description>
        <pubDate>Sat, 24 Aug 2013 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2013/08/24/Feebate-Program/</link>
        <guid isPermaLink="true">http://localhost:4000/2013/08/24/Feebate-Program/</guid>
        
        <category>Predictive Modeling</category>
        
        
      </item>
    
      <item>
        <title>Simulating an Epidemic: Yellow Fever Modeling in Veracruz, Mexico</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction:&lt;/h3&gt;

&lt;p&gt;The premise of this project is to illustrate the significance of model building and System Dynamics for epidemics - an applicable realization as the rapid spread of infectious diseases is a common trope throughout history. In fact, many epidemiologists predict that a pandemic the scale of the Spanish influenza will soon sweep the globe (COVID-19). Yellow Fever is transmitted through mosquitos, specifically the Aedes aegypti species. Monitoring the infected is complicated; this viral disease has different impacts on each person, making predictions on the duration and severity of Yellow Fever difficult to pinpoint.&lt;/p&gt;

&lt;p&gt;The models below simulate the proliferation of Yellow Fever through varying scenarios and mitigation techniques. While each case offers a new lens on predicting the spread of the viral disease, they all follow a similar backstory: urban diaspora. Individuals will contract Yellow Fever and bring it back to a populated urban center. If the infected person is bitten by a mosquito not carrying the disease, that mosquito will now become contagious.&lt;/p&gt;

&lt;p&gt;These simulations are adaptations of a Dynamo model for Yellow Fever outbreaks by Kjell Kalgraf. The figure below is the blueprint for this project’s modified simulations.&lt;/p&gt;

&lt;h2 id=&quot;kalgrafs-model&quot;&gt;Kalgraf’s Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/kalgraf-model.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kalgraf’s model is based on a real outbreak of Yellow Fever in Veracruz, Mexico (1899). The  population of Veracruz was nearly 20,000 people at the time. As the viral disease swept through the urban center, many casualties were suffered - 460 people died every month at the peak of the epidemic. The model predicts the mosquito population to be roughly 500,000. Kalgraf uses this outbreak as a hypothesis of sorts; he garners data based on population sizes, lifecycle of mosquitos, estimated bites per day, and the rates of contraction and recovery, to model the severity of Yellow Fever. Ultimately, this is shown through the comparison of cumulative number of deaths and the surviving, immune population.&lt;/p&gt;

&lt;p&gt;The parameters set for mosquitos are as follows. 27,778 mosquitos emerge into adulthood per day. At the height of the epidemic, there are a total of 500,000 mosquitos. The lifespan of a mosquito is 18 days; 4 ‘blood feeds’ are needed throughout their lifetime. Ergo, the average mosquito feeds off 0.2 persons per day - leaving roughly 100,000 bites happening per day. It’s discovered that if mosquitos don’t bite humans who are infected within their first three days, they are incapable of carrying Yellow Fever for the remainder of their lifetime. However, if mosquitos feed off an infected person within the first 3 days (the initial period) of contracting Yellow Fever, they will carry the disease.&lt;/p&gt;

&lt;p&gt;The parameters set for humans are a little more straightforward. A human can only contract Yellow Fever if they are bitten by a mosquito carrying the disease - it cannot be spread by another human. There are three stages to monitoring a disease: Incubation period, Contagious period, and Sick period. An infected person will enter an Incubation period (the # of days between infection and signs of symptoms) for 4.5 days. This is followed by a Contagious period of another 4.5 days, and a Sick period lasting 2.5 days. 90% of humans will recover and become immune to Yellow Fever.&lt;/p&gt;

&lt;h3 id=&quot;the-feedback-loops-in-an-epidemic-model&quot;&gt;The Feedback Loops in an Epidemic Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/feedback-loop.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are two feedback loops in the Yellow Fever model, both of which are Net Negative. The smaller loop on the far right alters the dynamics of the model by adding a fraction of people who are vulnerable to Yellow Fever. The larger loop to the left takes the mosquito population as a whole into account - the mosquitos that newly emerge into adulthood, and the infection rates among humans as a result. Both an increase in vulnerable people and an increase in mosquito will have a negative result on the model.&lt;/p&gt;

&lt;h3 id=&quot;modeling-yellow-fever-based-on-kalgrafs-predictions---the-human-population&quot;&gt;Modeling Yellow Fever Based on Kalgraf’s Predictions - The Human Population&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/yellow-fever-human.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This model is a subset of Kalgraf’s original model described prior, with emphasis on the human population. The population follows Kalgraf’s study of Veracruz - out of 20,000 people, 100 are ascribed the Incubated phase leaving 19,900 vulnerable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/vulnerable-vs-sick.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This time series model illustrates the human population’s exposure to Yellow Fever over 280 days. During the initial stages of the epidemic, there is an exponential increase in sick people as people carrying Yellow Fever are unaware in the first 4-5 days. However, as the population starts to incubate and follow proper isolation procedures, a linear decline in vulnerable people appears. The amount of sick people tapers off as the amount of vulnerable people declines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/immune-vs-deaths.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This time series model denotes Kalgraf’s overarching goal - to model the number of people who survive Yellow Fever and become immune, versus the number of cumulative deaths. It parallels the 90% survival rate of the disease; 18,000 people survive and become immune, and 2,000 people die.&lt;/p&gt;

&lt;p&gt;Analyzing just the human population in Kalgraf’s model allows for a more granular understanding of the implications associated with Yellow Fever outbreak rates and human recovery rates. For instance, the model assumes the amount of bites received by infected mosquitos per day will remain constant over time. In reality, the amount of bites per infected mosquito will oscillate; as the human population begins building immunity, the rate of infected mosquitos will decline - and the number of bites received per day will follow. Moreover, holding the stages of Yellow Fever (incubated, contagious, and sick) at finite increments poses issues. For example, recovery rates will differ among person to person. While using Veracruz, Mexico as a case study allowed for predicting the diaspora of Yellow Fever in urban centers, and a better understanding of human recovery rate with real data, it’s important to note the prevalence of the city’s climate for mosquito reproduction. The tropical climate with little variation from season to season is ideal for mosquitos. Places like Veracruz with high rates of precipitation and overall higher temperatures, will experience a greater possibility of infected mosquitos - and therefore higher rates of bites per day. Places with similar climates to this case study will be exposed to greater rates of Yellow Fever outbreaks.&lt;/p&gt;

&lt;h3 id=&quot;modeling-yellow-fever---simulating-kalgrafs-full-model&quot;&gt;Modeling Yellow Fever - Simulating Kalgraf’s Full Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/yellow-fever-full.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a working model of Kalgraf’s diagram in its entirety. The parameters set for humans follows the nuanced modeling of the ‘Human Population’ segment above - initialized with 100 incubating humans, and 19,900 vulnerable humans. The emerging mosquito flow in the model represents the 27,778 mosquitos Kalgraf predicted reach adulthood and can carry Yellow Fever. Out of the 500,000 mosquitos, 417,000 denote ‘safe mosquitos.’ The remaining 83,000 represent ‘new mosquitos.’ These numbers are estimates for easier understanding - they are not the specific numbers Kalgraf recorded in Veracruz, Mexico. Those nuanced statistics are stated below.&lt;/p&gt;

&lt;p&gt;To better understand Kalgraf’s model, a walk-through of formulas derived through the analysis of Veracruz’s Yellow Fever outbreak is provided below. The numbers in this breakdown refer to the first model introduced in the project.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;781 people are in the Incubation Stage, 803 people are in the Contagious Stage, and 450 people are sick. The peak of the Yellow Fever outbreak occurs around day 140, with nearly 450 people becoming sick. The highest volume of deaths per day is 18 people.&lt;/li&gt;
  &lt;li&gt;83,000 mosquitos are characterized as ‘new mosquitos’ and have the potential of carrying Yellow Fever if they come into contact with an infected human. These mosquitos bite 0.2 people every day - a total reaching over 16,000 per day. Kalgraf predicted 4.2% of these mosquitos will come into contact with an infected individual, making nearly 700 of these ‘new mosquitos’ carriers of Yellow Fever per day.&lt;/li&gt;
  &lt;li&gt;7,909 mosquitos are set to the incubation stage; 1,945 are in the infectious stage. These 1,945 infectious mosquitos are the catalyst for the peak in Yellow Fever outbreak on day 140.&lt;/li&gt;
  &lt;li&gt;8,103 humans are categorized as ‘vulnerable’ for the outbreak. To calculate the number of people contracting Yellow Fever, the following formula was used:
 &lt;strong&gt;# of Contagious Mosquitos * Bites/Day * Percent of Vulnerable Human Population&lt;/strong&gt; or [1,945 * 0.2 * 42.6] = 166 persons per day.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The graphs below offer visual aide to Kalgraf’s model through the lens of 3 scenarios: Cumulative Deaths (humans) vs Vulnerable Population, Cumulative Death during the outbreak, and Infectious Mosquitos vs Cumulative Deaths. They serve as visual representations for the statistics recorded above.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cumulative-vulnerable.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model above showcases a simulation for cumulative deaths and vulnerable people during the Yellow Fever outbreak.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/deaths.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model above illustrates simulated values for the number of casualties experienced during the Yellow Fever outbreak. The peak number of deaths experienced per day is 18 people and occurs around day 140 of the 280-day outbreak.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mosquitoes-vs-death.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model above represents the number of mosquitos carrying Yellow Fever and the number of deaths at the peak of the outbreak. At the peak of the outbreak, 1,945 mosquitos are infected.&lt;/p&gt;

&lt;h3 id=&quot;sensitivity-analysis---alterations-in-bites-per-day&quot;&gt;Sensitivity Analysis - Alterations in Bites Per Day&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sens-analysis-sick-people.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sensitivity Analysis graphing allows the viewer to inspect the impact of various bites per day per mosquito. The values used oscillate between 10% - 50%. The graph produced illustrates the change in number of sick people, as the number of mosquito bites occurring per day changes. The relationship between bites per day and number of infected people mirror one another. When the initial value of bites per day increases, the amount of sick people increases.&lt;/p&gt;

&lt;h3 id=&quot;reducing-the-mosquito-population---implementing-a-larvicide-program&quot;&gt;Reducing the Mosquito Population - Implementing a Larvicide Program&lt;/h3&gt;

&lt;p&gt;The application of a Larvicide Program assumes a reduction in the number of emerging mosquitos. It does not reduce the number of pre-existing adult mosquitos.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/larvacide.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Larvicide Program is represented as a Converter in the diagram above. It is directly associated with the ‘Emerging Mosquitos’ flow. 27,778 represents the ‘Emerging Mosquitos’ population Kalgraf discovered in Veracruz. To visualize the implications of a possible Larvicide Program on this population, the equation for the original flow needs to be changed. The equation for this flow now represents: 27,778 - (27,778 * [Larvicide Program]). The flow equation was then altered to 27,778 – (27,778 * [Mosquito Control Program]). Pause intervals and sliders were set for 10 days and 20 days, allowing a buffer for the days without a mosquito control program to be imagined.&lt;/p&gt;

&lt;p&gt;The graphs below illustrate the importance of timing when implementing the Larvicide Program. The success of the Larvicide Program was defined as limiting the number of people impacted by Yellow Fever as 500 or less. The first graph represents a 10-day hiatus on the population, and second graph represents a 20-day hiatus. Both simulations alter the effectiveness of the Larvicide Program on the same scale: 70%, 80%, and 90%. It’s implied that a 100% effective Larvicide Program would eliminate all ‘Emerging Mosquitos.’&lt;/p&gt;

&lt;h4 id=&quot;a-proposed-mosquito-control-program-10-day-simulations-vs-20-day-simulations&quot;&gt;A Proposed Mosquito Control Program: 10-Day Simulations vs. 20-Day Simulations&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/10-Day-Sim-Mosquito-Program.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regardless of the intensity of effectiveness set for the Larvicide Program, the ranges 70% - 90% passed the 500 people or less becoming infected by Yellow Fever success rate of the Program. As one would imply, the higher the magnitude of effectiveness, the less people there were becoming sick. 10 days served as effective parameter to set for the enactment of the Larvicide Program, giving realistic measurements for its hypothetical implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/20-Day-Sim-Mosquito-Program.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The 20-day graph above tells the viewer how vital the intial start time of the Larvicide Program is. Although the intensity of effectiveness for the three scenarios remained the same - 70%, 80%, and 90% - the 20-day hiatus caused a failure in one of the scenarios. If 20 days pass, and a 70% effective Larvicide Program were applied to the ‘Emerging Mosquitos,’ 550 people would become sick. This does not meet the standards required for the Program’s success. Both an 80% and 90% effective Larvicide Program keep the sick population under 500 people.&lt;/p&gt;

&lt;h3 id=&quot;a-proposed-isolation-program-the-importance-of-cooperation-in-a-10-day-simulation&quot;&gt;A Proposed Isolation Program: The Importance of Cooperation in a 10-Day Simulation&lt;/h3&gt;

&lt;p&gt;This hypothetical Isolation Program assumes two possibilities: it’s possible to detect Yellow Fever during the 4.5-day Incubation Period, and it’s possible to isolate all infected individuals before they become contagious. The success of this Isolation Program follows the same criteria as the Larvicide Program: The number of people impacted by Yellow Fever must be 500 or less. The effectiveness of the Isolation Program will range from 40% - 60% (parameters I thought seemed realistic, considering the mixed cooperation among people ordered to stay inside during an epidemic).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/isolation.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kalgraf’s original model has been modified to include this hypothetical Isolation Program. A newly created ‘Isolation’ flow links the pre-existing ‘Incubating People’ Stock to a newly created ‘Isolated People’ Stock. The ‘Percentage Isolated’ Converter allows for the modification of effectiveness among the three Isolation Program scenarios.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/10-Day-Isolation.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regardless the magnitude of the Isolation Program effectiveness, all scenarios successfully limited the number of impacted people under 500. A 60% effectiveness of the Isolation Program had a peak number of sick people at 300. A decline in 10% cooperation at the 50% simulation allowed for 100 more people to be impacted by Yellow Fever. The highest margin of people impacted by the disease was when less than 50% of the populated cooperated with the Isolation Program.&lt;/p&gt;

&lt;h3 id=&quot;epidemic-control-programs---larvicide-vs-isolation&quot;&gt;Epidemic Control Programs - Larvicide vs Isolation&lt;/h3&gt;

&lt;p&gt;The Isolation Program offered a safer solution to reducing the number of the people impacted by Yellow Fever, but it relied on the contingency that people would adhere to the mandated stay-at-home order. For the Larvicide Program to be successful, the insecticide needed to be extremely potent - and high rates of the chemical could potentially harm the human population. The safest protocol would be the Isolation Program, which exemplified a quicker recovery rate (80 days) than any of the simulations for the Larvicide Program.&lt;/p&gt;

</description>
        <pubDate>Fri, 23 Aug 2013 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2013/08/23/Yellow-Fever/</link>
        <guid isPermaLink="true">http://localhost:4000/2013/08/23/Yellow-Fever/</guid>
        
        <category>Predictive Modeling</category>
        
        
      </item>
    
      <item>
        <title>Modeling the Oasis of the Dry Great Basin: Predicting Water Values of Mono Lake</title>
        <description>&lt;h3 id=&quot;a-basic-understanding-of-mono-lake&quot;&gt;A Basic Understanding of Mono Lake&lt;/h3&gt;

&lt;p&gt;Los Angeles began diverting water from Mono Lake in 1941. The goal of this predictive model is to examine the effect of different export policies on the elevation of Mono Lake. The differing models will project future sizes of the lake given different assumptions about the amount of water exported to Los Angeles. Through this analysis, the viewer will be able to ascertain how the lake level responds to changes in export policies - as well as derive a sustainable level of export. This viable tradeoff between lake level oscillations and tradeoff policies will be realized when the Mono Lake model indicates a stable equilibrium.&lt;/p&gt;

&lt;p&gt;The figure below depicts historical elevations of Mono Lake:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mono-lake-elevation.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;modeling-mono-lake---a-simplistic-model&quot;&gt;Modeling Mono Lake - A Simplistic Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mono-lake-1.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This model illustrates a basic understanding - a simple interpretation of the varying inputs and outputs that dictate the amount of water in Mono Lake. However, as the viewer will see with the resulting graph below, this model is far too simplistic to produce tangible results.&lt;/p&gt;

&lt;p&gt;The parameter values for the model are as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/parameter-values.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The volume of water in Mono Lake is measured in thousands of acre-feet (KAF). The model is run by years, so the units for the flows in the model will be KAF/year. Precipitation and Evaporation parameters will be measured by feet/year. Precipitation is calculated by multiplying the surface area by the precipitation rate. Evaporation is calculated in the same manner - surface area * evaporation rate. The surface area is set to 39,000 acres (39K acres), the same as it appeared in 1990.&lt;/p&gt;

&lt;p&gt;The flow of water in Mono Lake is achieved by subtracting ‘Export’ from the Lake’s main flow of water - the runoff from the Sierra Nevada.&lt;/p&gt;

&lt;p&gt;The Stock of the model is the amount of water in the Mono Lake.&lt;/p&gt;

&lt;p&gt;The Inputs include: Runoff from Sierra Nevada, Precipitation onto the lake, and an ‘Other Out’ variable that includes ungauged runoff and municipal flows.&lt;/p&gt;

&lt;p&gt;The Outputs include: Evaporation from the lake’s surface, an ‘Export’ variable that denotes the Los Angeles Aqueduct, and an ‘Other Out’ variable for remaining possible influences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/graph-linear.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The simulation results from this model indicates water’s decline from Mono Lake in a linear fashion. In fact, with these parameters, water is projected to reach zero by 2030 - and continually decline below this “zero line” to -600 by the 2042.&lt;/p&gt;

&lt;p&gt;Given the fact that the water in Mono Lake would likely never be in the negative, we can conclude this model is too simplistic. A strictly linear decline does not make for a reliable simulation; the stock-and-flow diagram will need to be expanded to alter its structure and deliver reliable results.&lt;/p&gt;

&lt;h3 id=&quot;modeling-mono-lake---a-more-realistic-model&quot;&gt;Modeling Mono Lake - A More Realistic Model&lt;/h3&gt;

&lt;p&gt;In reality, Mono Lake’s surface area should shrink as the volume of water in the lake declines. This model takes a more realistic approach by altering Surface Area into an internal variable - one that will parallel a decline in water volume.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/survey.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Geological and bathymetric surveys were conducted to calculate the areas associated with varying volumes of water in Mono Lake. Elevation of the lake with the differing volume levels is also included in the survey. From a geographic standpoint, knowing the elevation of the lake is helpful because it indicates a plethora of conditions - like increasing exports impact on the lake. Looking at the extremes, no water in the lake would indicate 6,224 feet above sea level… and 8,000 KAF in the lake would be 6,477 feet above sea level.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mono-lake-model-2.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This new model of Mono Lake includes the survey results listed prior. As stated above, ‘Surface Area’ needs to be an internal variable for our model. This variable, along with the newly added ‘Elevation’ variable from the survey will take the form of Converters and be directly tied with the amount of water in Mono Lake.&lt;/p&gt;

&lt;h3 id=&quot;modeling-mono-lake---the-final-model&quot;&gt;Modeling Mono Lake - The Final Model&lt;/h3&gt;

&lt;p&gt;After careful observation, it’s decided the missing factor for predicting the most accurate model is an accurate change in the rate of evaporation. Mono Lake is a saline lake, meaning the rate of evaporation is slower than fresh lakes. The slower rates of evaporation associated with saline lakes is caused by a reduction in vapor pressure difference between the surface of water and the overlying air. An accurate evaporation rate must take into consideration the large quantities of fixed solids dissolving in the lake’s shrinking volume of water.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/specific-gravity.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Evaporation studies, also called ‘Pan Studies’ as pans of water with differing salinities are exposed to evaporation, allow us to present a more realistic understanding of Mono Lake’s evaporation rate in association with gravity. The density of water in the lake is represented by ‘Specific Gravity’ - a measurement that compares the density of saline water to fresh water. For instance, a value of 1.1 indicates Mono Lake’s water is 10% heavier than fresh water. The impact of having a higher salinity for evaporation rates is represented by ‘Effect on Evaporation Rate.’ If the water in Mono Lake is 10% heavier compared to fresh water, for instance, the corresponding evaporation rate would be 92.6% of the rate for fresh water. Through this chart, we see as ‘Specific Gravity’ increases, the ‘Effect on Evaporation Rate’ decreases.&lt;/p&gt;

&lt;p&gt;‘Specific Gravity’ is calculated through the following formula:
	&lt;strong&gt;(Water in Lake * Density of Fresh Water + Total Dissolved Solids) / Water in Lake * Density of Fresh Water&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The simulation begins with 2,228 KAF of water in the lake. The density of fresh water is 1.359 million tons per KAF, and the total dissolved solids is 230 million tons.&lt;/p&gt;

&lt;p&gt;‘Specific Gravity’ = 2,228 KAF * 1.359 Million Tons per KAF + 230 Million Tons / 2,228 KAF * 1.359 Million Tons per KAF
				 = 1.076&lt;/p&gt;

&lt;p&gt;Interpolating from the chart above, we can estimate a Specific Gravity of 1.076 is an evaporation rate of about 94% of the evaporation rate for fresh water.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/model-2-and-3.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This new model of Mono Lake indicates that the added ‘Specific Gravity’ variable has a direct correlation on the evaporation rate. The ‘Specific Gravity Effect on Evaporation Rate’ Converter in the model above represents this relationship. Internal variables like ‘Total Dissolved Solids’ and ‘Density of Fresh Water’ have also been added and linked to the ‘Specific Gravity’ variable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/model-3-graph1.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/model-3-graph2.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The time graphs above represent 4 variables: Water in Lake, Elevation, Specific Gravity, and Surface Area. The specific gravity simulation (graph 2) begins at 1.076, mirroring our calculation above. The simulation concludes at 1.163. The pattern for elevation (graph 1) parallels the movement of our second simulation for Mono Lake - a gradual decline that eventually reaches dynamic equilibrium. At the end of the simulation, the elevation is 6,336 feet above sea level. That’s a 16-foot difference in sea level than in our previous model.&lt;/p&gt;

&lt;h3 id=&quot;modeling-mono-lake---a-simulated-buffer-policy-enacted&quot;&gt;Modeling Mono Lake - A Simulated Buffer Policy Enacted&lt;/h3&gt;

&lt;p&gt;The Model below simulates an export similar to a real proposal by the Mono Lake Committee. The goal of this simulation is to come up with a realistic range for lake elevation that guarantees the ecosystem’s safety. The difference in models is the pre-existing ‘Export’ variable has been changed to a Converter and linked with the ‘Elevation’ Converter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/model-4.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/model-5.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If the elevation is below 6,380 feet, exports are not allowed. If the elevation is above 6,390 feet, 100 KAF/yr is allowed. If the elevation is within our realistic range - a buffer zone, of sorts - export will change in a linear manner.&lt;/p&gt;

&lt;h3 id=&quot;modeling-mono-lake---a-simulated-control-board-policy&quot;&gt;Modeling Mono Lake - A Simulated Control Board Policy&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/policy-graph.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph above illustrates the impact of the California Water Resources Control Board’s export policy in 1994. The intention of this policy was to allow Mono Lake to reach an elevation near 6,392 feet. This would happen over many years and throughout various stages - hence the jagged shape of the graph. The policy described is as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No export allowed until Mono Lake had an elevation of 6,377 feet&lt;/li&gt;
  &lt;li&gt;Once at 6,377 feet, exports allowed at 4.5 KAF/yr until elevation reaches 6,390 feet&lt;/li&gt;
  &lt;li&gt;Exports allowed at 16 KAF/yr until elevation reaches 6,391 feet&lt;/li&gt;
  &lt;li&gt;Exports allowed at 30.8 KAF/yr if elevation exceeds 6,391 feet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The simulation of this Export Policy is the graph above. Through its progression over a sufficiently long period of time, 100 years, we can deduce the future success of Control Board Policy. If the parameters set for exports of a given elevation level are maintained, the lake will reach dynamic equilibrium.&lt;/p&gt;

</description>
        <pubDate>Thu, 22 Aug 2013 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2013/08/22/mono-lake/</link>
        <guid isPermaLink="true">http://localhost:4000/2013/08/22/mono-lake/</guid>
        
        <category>Predictive Modeling</category>
        
        
      </item>
    
      <item>
        <title>S-Shaped Growth: Modeling the Natural System of a Flowered Area</title>
        <description>&lt;h4 id=&quot;flower-model---intrinsic-growth-vs-actual-growth-rate&quot;&gt;Flower Model - Intrinsic Growth vs. Actual Growth Rate&lt;/h4&gt;

&lt;p&gt;This model explores one of the six types of dynamic patterns: S-Shaped Growth graphing. S-Shaped graphs are crucial for describing patterns in natural systems. This project simulates the spread of flowers across a 1,000-acre landscape.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flower-model1.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are two stocks in the model above: “Flowered Area” and “Empty Area.” The initial value for the “Flowered Area” is 10 acres; the “Empty Area” is 990 acres. With 10 acres purely of flowers, there will be minimal limitations hindering their spread - the seeds will have open space to grow in all directions. The variable “Decay Rate” is set to 20% per year and is the product of the “Flowered Area” and rate of decay. The growth of the “Flowered Area” is dictated by the “Growth Rate” variable (a product of the Growth Rate and Flowered Area).&lt;/p&gt;

&lt;p&gt;If the model above only took the stated variables for simulation, the flowered area would expand by 80% per year. With no resource limitations, the model would assume an Intrinsic Growth Rate. To develop a more realistic model of the natural spread of flowers, an “Actual Growth Rate” variable will be introduced to the simulation. The “Actual Growth Rate” will be less than the Intrinsic Growth Rate of our first scenario - as the flowers spread across the landscape, the amount of “Empty Area” available will decrease and the flowers’ possibility to germinate will be increasingly harder.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flower-model1-graph.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph above illustrates a remarkably quick spread of the “Flowered Area” within the first 10 years, but a dynamic equilibrium at 800 acres - not the full 1,000 acres available in the landscape. The “Flowered Area” starts with 10 acres, and within 6 years is covering 50 times its original breadth at a whopping 500 acres. This 6th year on the graph indicates the highest rate of growth in the “Flowered Area” as the growth exceeds the decay by the biggest margin. After this, the increasing trends starts to stabilize and eventually tapers off - an inverse of the first 6 years with a new decline in growth and increase in decay. Dynamic Equilibrium is met around the 12th year, 4/5th of the landscape completely covered in flowers. The flowers will never cover the entire landscape, because the “Decay Rate” is fixed at 20% throughout the simulation.&lt;/p&gt;

&lt;h3 id=&quot;introducing-a-disturbance---the-implications-of-equilibrium&quot;&gt;Introducing a Disturbance - The Implications of Equilibrium&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flower-disturbance.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flower-disturbance-graph.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This model allows for a disturbance to remove 20% of the flowers. The “Disturbance” Converter is linked with a newly added “Disturbance Flow” that will enact on the 15th year of the simulation. The “Total Area” internal variable has been changed to a Stock in the model. The model was set for a duration of 20 years to indicate if equilibrium would be reached, but the added “Disturbance” only reduced the “Flowered Area” for the year it was enacted. The flowers gradually reached back to their apex of 800 acres 5 years after the “Disturbance” occurred.&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Aug 2013 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2013/08/21/s-shaped-growth/</link>
        <guid isPermaLink="true">http://localhost:4000/2013/08/21/s-shaped-growth/</guid>
        
        <category>Predictive Modeling</category>
        
        
      </item>
    
      <item>
        <title>Mendocino Fire Complex: Calculating Surface Vegetation Fluctuations through Spectral Indices (NDVI &amp; NBR)</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;This research delves into the complexities associated with measuring fire’s impact on an ecosystem, through the scope of vegetation health and re-growth. Change-detection mapping allows for an analysis of time variation to come to surface — an opportunity that provides a multi-faceted observation for comprehending the intricacies of disturbance patterns. In situ observations are a large aspect in the calculation of vegetation change post-fire; however, the application of various remote sensing techniques permits a less constricted, more versatile manifestation of methods for detecting (and understanding) complications associated with disturbance patterns.&lt;/p&gt;

&lt;p&gt;The flexibility associated with remote sensing techniques - specifically through the lens of vegetation alteration post-fire - is perceptible through two essential applications: The ability to obtain, calculate, and analyze data that transverses both a wide timespan, and/or span distances not feasible for direct analysis of study sites. This study explores fluctuations in the condition of surface vegetation, the Normalized Difference Vegetation Index (NDVI), of Mendocino County and the Mendocino National Forest before, during, and after the emersion of the Mendocino Fire Complex. Landsat 8 OLI/TIRS C1 Level-1 imagery was amassed, and several composites were produced to analyze NDVI values spanning the time before and after the Mendocino Fire Complex.&lt;/p&gt;

&lt;p&gt;In addition, this research uses satellite data to classify the fire severity of the burned areas in Mendocino County and the Mendocino National Forest. Fire severity was calculated through the spectral index of the Normalized Burn Ratio (NBR), which utilizes reflectance data directly before and shortly after the emergence of a fire. Pre-Processing steps were ensured - like creating water masks and radiometric calibrations - to limit the amount of interference. Highlighting the burned area of the Mendocino Complex and estimating overall burned severity was more complicated than its NDVI counterpart. Two approaches were taken - ENVI and Google Earth Engine (UN-SPIDER). Both procedures are described in full below; however, once a comparison of data values became palpable, the results derived through Google Earth Engine were selected for representation.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction:&lt;/h3&gt;

&lt;p&gt;The Mendocino Fire Complex emerged from two fires near Mendocino County, the River Fire and the Ranch Fire, and coined the title of California’s largest wildfire in state history (2018) by devouring roughly 450,000 acres. These twin flames destroyed upwards of 280 edifices, an estimated 160 being homes, and charred an extent larger than Los Angeles.  The Mendocino Fire Complex (here on denoted as ‘Complex’), surfaced on July 27th, 2018 at 12:05PM, and lasted for months until its containment on October 4th, 2018 at 10:10AM. The continuation of the Complex is most likely attributed to California’s Mediterranean climate; the state’s triple digit temperature average in the month of August cultivates the necessary conditions for a heat wave to persist and allow the commencement of wildfires. This fact, coupled with the knowledge of high winds’ capability of propelling increased amounts of timber and dry vegetation, alludes to the multi-directional dispersion of the Complex. The Complex’s shift towards the USFS Mendocino National Forest is of prime examination in this report — an emphasis that underscores the dangers and necessities of fire in an ecosystem teeming with diverse biogeographic and physiographic qualities, and exemplifies the impact of the Complex on a region formerly characterized as having substantially high NDVI surface values.&lt;/p&gt;

&lt;p&gt;California’s dry summers are kindled by two factors: Cold ocean currents, and a strong diurnal character to daily temperatures. Cold ocean currents are the initial force held accountable for the dry weather experienced during California’s summers — this high-pressure system hinders the ability of cloud formation and resulting rainfall. The amplification of sunlight during this season intensifies the degree of heating during the day, coupled with increased rates of cooling during the night, manifests a strong diurnal character to daily temperatures that ensures the requirements necessary for a certain degree of dryness to persist. This absence of precipitation can linger anywhere between the duration of three to six months. The retreatment of cold ocean currents during the winter, allows for warm oceanic water to amass near the shoreline. This increased rate of warm water creates the required conditions for precipitation; as hot air continues to rise in the atmosphere, it begins to expand and consequently cool. When the air molecules begin to cool, relative humidity intensifies and causes the water particles to condense. The result is increased cloud formation and likewise increased precipitation rates.&lt;/p&gt;

&lt;p&gt;Temperatures in regions scattered throughout the state of California are greatly influenced by the distance from large bodies of water. However, due to the relative closeness of most areas near these large bodies of water, the temperature ranges of a Mediterranean climate are moderate; the range in temperature amid the winter season’s high and the summer season’s low is miniscule. The greatest impact on the summer’s daily range of amplified temperature values is the exaggeration of the season’s dry, cloud-free settings. Factors such as latitude and elevation play an essential role in the summer season’s temperature range from mild to very hot, but generally the farther an area is from a large body of water, the higher the range in temperature. Regions of greater distance from larger bodies of water are at a greater risk of wildfire development during the summer season, as intensified winds stemming from the inland, arid regions enhance average temperature values.&lt;/p&gt;

&lt;p&gt;In 2018, the state of California experienced a substantial duration of drier than average temperature conditions. The region’s most at-risk of increased wildfire emergence were constricted between Southern and Central California, where reports of a 25% decrease in monthly precipitation values were deduced. Values of peak dryness were actualized in the mid-months of California, which established the preliminary conditions necessary for wildfire occurrence. The state’s long-lasting drought ensued throughout November, with rates intensifying throughout the Mendocino National Forest.  This accentuation of increasing temperature values and declining precipitation values throughout the Mendocino National Forest is of prime importance for this study, which investigates the changes in surface vegetation after the Mendocino Fire Complex.&lt;/p&gt;

&lt;p&gt;Forests are one of the world’s most complex and comprehensive ecosystems, able to prosper globally in varying degrees of weather and states of existence, and home to countless species and priceless resources. While forests account for a marginal amount of the Earth’s total surface (14%), when paired with savannahs they comprise the highest quantities of organic material compared to any other global ecosystem, and are responsible for upwards of 40% of total solar energy absorbed each year by green vegetation. Forests are associated with having the highest rates of biodiversity throughout all terrestrial ecosystems, meaning the largest quantities of biological resources, and thus are instrumental in sustaining the Earth’s ecological balance. The warm summers in California place forests in a precarious situation when balancing the importance and risk associated with wildfires in an ecosystem of global importance. This study monitors the fluctuations in NDVI values across the Mendocino National Forest before and after the largest forest disturbance in state history, in an attempt to better understand land ecosystem patterns of regeneration.&lt;/p&gt;

&lt;p&gt;While this research is focused on the local lens of the Mendocino National Forest, it is important to mention the influence of largescale forest disturbances on a regional and global level. Forest disturbance (and likewise forest regeneration) is a crucial factor in the transfer of carbon between the atmosphere and the surface. Moreover, these disturbances are responsible for altering the landscape of a forest, manipulating the degrees of energy associated with the ecosystem’s biogeochemical cycle, as well as establishing the biological and physical characteristics behind a forest’s overall makeup.&lt;/p&gt;

&lt;p&gt;The Mendocino National Forest promotes roughly 2.6 million acre-feet of water per year, equivalent to about 847 billion gallons per year. This is especially prevalent due to the increased propensity of California with drought-likeliness, as mentioned prior with intensified temperatures during the summer season. This National Forest has a radius of 233,704 acres of wilderness, and stores upwards of 168.5 million metric tons (mmt) of Carbon Dioxide in its soil. The objective of this project is to determine how the Mendocino National Forest — and surrounding areas of direct impact — surface vegetation is influenced by the Mendocino Fire Complex. This study accentuates the importance of recognizing temporal and spatial trends (and anomalies) in the supervision of forest health, by monitoring the occurrence and distribution of a forest fire disturbance within the Mendocino National Forest. Ultimately, this research denotes the importance of observing changes in vegetation after forest disturbances such as wildfires, through the underlying message of implementing more advanced management strategies and tools of observation to mitigate future damages.&lt;/p&gt;

&lt;p&gt;Although preceding studies on vegetation monitoring after a forest disturbance rely heavily on data observed from in situ observations, W. Chen et al. showcases the ability and assurance of observing vegetation changes through the lens of remotely sensed data. The utilization of remote sensing in their study underscores the ineffectiveness and costliness of a not-always-standardized ground survey. W. Chen et al. holds that there is an increased necessity for remotely sensed estimates of burned forest areas, especially if forest management and assessment is to be advanced in future studies. Countless studies on forest disturbance mapping methods indicate that remote sensing is the most effective technique of regularly monitoring forest dynamics at large scales. Satellite remote sensing represents a cost-friendly alternative for forest change monitoring in areas of great expanses.&lt;/p&gt;

&lt;p&gt;Data provided by remote sensing techniques has offered a multifaceted approach at mapping the various sizes of forest disturbances from wildfires, such as varying algorithms of supervised and unsupervised classification. However, this study uses threshold segmentation based on single-band reflectance of an essential vegetation index, the Normalized Difference Vegetation Index (NDVI), to precisely abstract data on the burned area of the Mendocino National Forest and its surrounding boundaries. This analysis is provided from Landsat 8 OLI/TIRS C1 Level-1 imagery. Similar to the results of W. Chen et al., this study found that a forest’s most densely vegetated areas experienced the largest decrease in NDVI values.&lt;/p&gt;

&lt;p&gt;For comparison in aptitude of measurement and a dual understanding of differing methods utilized to calculate a forest disturbance’s impact on surface vegetation, fire severity was calculated through the spectral index of the Normalized Burn Ratio (NBR). NBR utilizes reflectance data directly before and shortly after the emergence of a fire. Fire severity is calculated remotely using spectral indices of single-date and multitemporal index data [6] — [12]. NBR is is calculated by obtaining the difference between near-infrared (NIR) and short-wave infrared (SWIR) reflectance, divided by their sum.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Mendo-GIS.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This reference map of the Mendocino National Forest and the Mendocino Fire Complex was created entirely in ArcGIS. Shapefiles containing the perimeters of the Mendocino Fire Complex’s two fires were downloaded from the Geospatial Multi-Agency Coordination (GeoMac), powered by USGS. Shapefiles containing the boundaries of Mendocino County and Mendocino National Forest were also downloaded. The Mendocino County shapefile was provided by the US Census Burau – 2018 TIGER/Line, and the Mendocino County shapefile was provided by the USDA Forest Service. Some of the rivers and roads within Mendocino County fell within the boundaries of the burned areas; to reduce the visual impact on mapping the burned regions of Mendocino County and Mendocino National Forest, the rivers and roads intersecting the burned areas were extracted and then masked.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NDVI Data Acquisition and Statistical Analysis:&lt;/strong&gt;
Due to the Mendocino Fire Complex’s prolonged duration of commencement - July 27th to October 4th - two Landsat TM images were obtained before the fire emerged and after the fire was extinguished (path 45 row 33: July 10; path 45 row 32: July 10; path 45 row 32: October 30; path 45 row 33: October 30). These Landsat images were provided by United States Geological Survey Earth Explorer, and were determined to be the best representation of satellite images for calculating NDVI values based on area covered and lack of cloud interference. The composites acquired from USGS Earth Explorer contain a spatial resolution of 250 meters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Mendo-July10.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above is the Landsat imagery obtained before the Mendocino Fire Complex, with emphasis on the Mendocino National Forest (July 10, 2017).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Mendo-Oct30.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above is the Landsat imagery obtained after the Mendocino Fire Complex, with the same scale for reference (October 30, 2017). An apparent decrease in vegetation is realized.&lt;/p&gt;

&lt;p&gt;Vegetation Indices are groupings of single bands captured by a remote sensing image, and generally associated with the effectiveness of characterizing the status of surface vegetation. Normalized Difference Vegetation Index (NDVI) captures one of the most wide-ranging illustration of surface conditions, due to its combination of normalized difference formula and use of regions of chlorophyll that exhibit the highest rates of reflectance and absorption.  NDVI is generalized as the most frequently used vegetation index for characterizing vegetation conditions, because it uses countless bands of remotely sensed data. This Multi-Spectral Remote Sensing data technique helps to accurately define the classification of differing land covers, such as bodies of water or types of vegetation on a given surface.&lt;/p&gt;

&lt;p&gt;NDVI values are defined using the following equation: &lt;strong&gt;NDVI = (NIR - RED) / (NIR + RED)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;NIR and RED denote the reflectance of the near-infrared and red bands, correspondingly. NDVI values fall between a range of -1 to 1. The wavelength range for the red band (RED) is between 600nm - 700nm; the wavelength range for the near-infrared band (NIR) is between 750nm - 1300nm; the wavelength for the green band is 550nm. These values are likewise classified into differentiating surface types. For instance, values of 0.1 or below represent sand particles or barren rock, while values ranging from  0.2 - 0.5 represent grasslands or areas dominated by sparse vegetation. Ultimately, values greater than 0.5 represent dense vegetation cover types, often symbolic of forests.&lt;/p&gt;

&lt;p&gt;The utilization of an image analysis software, ENVI, was of key importance in determining NDVI values of the Mendocino National Forest before and after the Complex. To calculate surface vegetation values prior to the Complex, the two TM Landsat images for July 10, 2018 were uploaded into the software. NDVI was calculated by the formula above. The two TM images were combined using ENVI’s ‘Seamless Mosaic’ option, which georeferenced the given paths and rows of the scenes. Through ENVI’s subset capabilities, the program calculated NDVI values were clipped to the fire boundaries of the River Fire and Ranch Fire. These steps were repeated for the two TM Landsat images retrieved after the Mendocino Fire Complex was extinguished (the October 30, 2018 scenes).&lt;/p&gt;

&lt;p&gt;To calculate the total change in NDVI values of the Mendocino National Forest, ENVI’s ‘Band Math’ operation was utilized. Assessing total change in NDVI, was calculated through the following equation: &lt;strong&gt;Float (1) - Float (2).&lt;/strong&gt;
Ultimately, obtaining the total change in NDVI is subtracting the post-fire NDVI values with the pre-fire NDVI values. The pre-fire NDVI values, post-fire NDVI values, and total change in NDVI values, were then clipped to the boundaries of the Complex’s extent to produce three separate maps of analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fire Severity Estimation (Normalized Burn Ratio):&lt;/strong&gt;
Because fire severity is most accurate directly before a fire has emerged, and directly after a fire was extinguished, different Landsat 8 images were retrieved from USGS Earth Explorer. One of these Landsat scenes was obtained a mere 4-days before the Complex emerged (path 45 row 33: July 23), and the other was obtained one-week after the Complex subsided (path 45 row 33: October 11). The Normalized Burn Ratio (NBR) is crucial is measuring burned areas of fire zones greater than 500 acres. The calculation for NBR bares similarities with the formula to estimate the normalized difference vegetation index (NDVI), except the NBR swaps the red band (RED) with the shortwave-infrared band (SWIR) wavelength.&lt;/p&gt;

&lt;p&gt;Calculating the Normalized Burn Ratio of the region posed more challenges than the Normalized Difference Vegetation Index. Two routes were chosen for comparison - ENVI and Google Earth Image (UN-SPIDER). The ENVI steps are described below; however, UN-SPIDER allowed for more promising results and provided a more realistic classification table for interpreting burn severity. For these reasons, UN-SPIDER was selected for representation of NBR.&lt;/p&gt;

&lt;p&gt;NBR values are classified using the following equation:
&lt;strong&gt;NBR = (NIR - SWIR) / (NIR + SWIR).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Healthy vegetation indicates a high reflectance in the NIR band, with low reflectance in the SWIR band. This notion is juxtaposed by dying vegetation - low reflectance in the NIR and high reflectance in the SWIR - and is often defined by regions scorched by fires. Ergo, high values of NBR denote healthy vegetation, and low values illustrate burned areas (or areas lacking vegetation). Regions not affected by burns are ascribed values close to zero. Analyzing the change in NBR (dNBR) values across an area - the Differenced Normalized Burn Ratio - is often performed on regions that recently experienced a fire and allows for an understanding of estimated severity. The change in NBR is calculated by subtracting post-fire NBR values with pre-fire NBR values. High estimates of dNBR illustrate intense damage from the fire; negative dNBR readings represent regrowth.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ENVI for NBR:&lt;/strong&gt;
Multispectral sensors like Landsat 8 have a near-infrared band between 0.76nm - 0.9nm, and a shortwave-infrared band between 2.08nm - 2.35nm. First, the Landsat image was imported into ENVI. Preprocessing steps were followed to guarantee the images were correctly masked and calibrated before calculating burn indices. Because the Mendocino National Forest and surrounding areas in the Mendocino County contain large bodies of water, like Clear Lake for instance, a water mask was created. This mask ensured that pixels within the large bodies of water would be excluded, removing any chance of interference with atmospheric correction or calibration. ENVI automatically ascribes the black background pixels in Landsat 8 scenes to values of ‘NoData.’ The water mask was generated through the creation of band-threshold region of interest (ROI), using the near-infrared (NIR) band. Because of water’s particularly low reflectance in the NIR region, the pixels associated with large bodies of water become black. Through ENVI’s ‘Add New Threshold Rule’ feature, a histogram highlighting the lowest level pixel values for water became palpable. The before-fire scene (July 23) had a minimum value of 5,109 pixels and a maximum value of 7,618 pixels. The after-fire scene (October 11) had a minimum value of 5,333 pixels and a maximum value of 7,618 pixels. When saved as a TIFF in ENVI, the ‘Inverse Mask’ option was selected to ensure the values of ‘NoData,’ and the ‘Data Ignore Value’ was ascribed the value of 0.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To produce spectral index images like NBR, the image must be calibrated for top-of-atmosphere (TOA) reflectance.  Pixel values will oscillate between 0 to 1, or 0 to 100. The Radiometric Calibration was calculated, with ‘Reflectance’ set as the Calibration type. The ‘Normalized Burn Ratio–Thermal’ was further investigated and calibrated to brightness temperatures. The Thermal Band 1 was set to a Radiometric Calibration, with ‘Brightness Temperature’ set as the Calibration type. A Layer-Stacked image was created through the combination of the newly created Post-Fire Reflectance layer and Post-Fire Calibrated Brightness Temperature layer.&lt;/p&gt;

&lt;p&gt;The Differenced Normalized Burn Ratio (change in NBR) was calculated to find the true change in NBR values before and after the Mendocino Fire Complex. The Differenced Normalized Burn Ratio was estimated by subtracting the post-fire NBR image (October 11) from the pre-fire NBR image (July 23). ENVI’s ‘Band Math’ tool allowed for this calculation. The expression &lt;strong&gt;Float (b2 - b1)&lt;/strong&gt; was entered, with ‘b2’ representing the Normalized Burn Ratio of the post-fire, and ‘b1’ representing the Normalized Burn Ratio of the pre-fire. A Differenced Normalized Burn Ratio with values less than -0.25 indicate high post-fire regrowth; values between -0.1 and 0.1 indicate unburned areas and low severity; values between 0.27 and 0.43 indicate moderate-low severity burning; values between 0.44 and 0.65 indicate moderate-high severity burning; and values greater than 0.66 indicate high-severity burning. This classification of burn severity was proposed by the United States Geological Survey (USGS).&lt;/p&gt;

&lt;p&gt;The data values produced through this simulation - albeit steps correct - did not result in a classification as accurate as Google Earth Engine (UN-SPIDER) classification for burn severity. For this reason, the results obtained through Google Earth Engine were selected as representation for the Differenced Normalized Burn Ratio.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Google Earth Engine (UN-SPIDER):&lt;/strong&gt;
The spatial processing extent of this study (refer to the Reference Map produced above) was defined through the previously stated ESRI-Shapefiles. The shapefiles were uploaded into Google Earth Engine (GEE) as ‘Table Assets.’ These files were then imported into the script provided by GEE. Landsat 8 was the designated sensor for creating the Burn Severity Mapping. GEE’s script has an algorithm that automatically masks clouds based on the sensor’s pixel quality. It also performs the stacking featured described in the manual process for ENVI, as it creates a composite image based on mosaics of the clipped area. This process allows for an atmospherically corrected image. The Normalized Burn Ratio before the Complex (July 23) and after the Complex (October 11) was calculated on the composite. The subtraction of these two states - pre-fire and post-fire - allowed for the creation of the Differenced Normalize Burn Ratio map seen below.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;NDVI Analysis - Before Fire&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Before-NDVI.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map illustrates the Normalized Difference Vegetation Index (NDVI) values prior to the Complex’s emergence (July 10, 2018). These values are derived from Landsat 8 imagery provided by USGS Earth Explorer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/before-fire-hist.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/before-fire-stats.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;NDVI values measured before the Mendocino Fire Complex on July 10, 2018 were discovered to be of moderately high values on the NDVI scale, revealing relative levels of healthy vegetation. Aforementioned in the introduction portion of this study, is the influence of California’s Mediterranean climate. The lack of precipitation values in the year 2018, coupled with the intensified temperature values in the state’s dry summer season, will not allow for NDVI values to be extremely high to begin with. Comparing these averages with NDVI values associated with vegetation in tropical climates like Puerto Rico, for instance. During this satellite image, the mean NDVI value was reported at 0.3288; the minimum was reported at -0.0700; and the maximum was reported 0.6639.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NDVI Analysis - After Fire&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/After-NDVI.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map illustrates the Normalized Difference Vegetation Index (NDVI) values after the Mendocino Complex (October 30, 2018). These values are derived from Landsat 8 imagery provided by USGS Earth Explorer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/after-fire-hist.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/after-fire-stats.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The impact of the Mendocino Fire Complex on NDVI values was a complete vegetation loss across areas directly impacted by the Complex, especially in highly vegetated areas like the Mendocino National Forest. During the date of this satellite image post-fire, October 11th, the mean NDVI value was reported at 0.0893; the minimum NDVI value was reported at -0.1442; and the maximum NDVI value was reported at 0.4487.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NDVI Analysis - Total Vegetation Change&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Total-NDVI.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map illustrates the calculated change in Normalized Difference Vegetation Index (NDVI) values across the burned areas after the Mendocino Complex’s dissipation. It was calculated by finding the difference of post-fire NDVI values and pre-fire NDVI values. These values are derived from Landsat 8 imagery provided by USGS Earth Explorer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/change-ndvi-hist.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/change-ndvi-stats.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The total impact of the Mendocino Fire Complex on NDVI values was a general decrease in vegetation levels across the board. The total change in the mean NDVI value was reported at -0.2395; the minimum change in total NDVI value was reported at -0.5641; and the maximum change in total NDVI value was reported at 0.2679.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Differenced NBR Analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/NBR.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This map illustrates the Differenced Normalized Burn Ratio (dNBR) values after the Complex’s emergence. The area is teeming with changes in NBR of all values, but it’s apparent to the viewer that most values fall within the ‘Moderate-Low Severity’ to ‘High Severity.’ This map was produced through Google Earth Engine’s UN-SPIDER application. The burn severity classification is provided by USGS.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion:&lt;/h3&gt;

&lt;p&gt;The objective of this research was to unveil the changes in vegetation cover over the Mendocino National Forest and surrounding areas in Mendocino County, after the emergence of California’s largest wildfire in state history, the Mendocino Fire Complex (2018). Moreover, its goal was to analyze the difference in calculating vegetation cover change through the comparison of Normalized Difference Vegetation Index (NDVI) values and the spectral indices of Normalized Burn Ratio (NBR). Overall, this goal was succeeded as both variables experienced a drastic decrease in areas where vegetation intersected the breadth of the Complex’s expanse. These conclusions are consistent with previously hypothesized outcomes of a forest disturbance’s influence on NDVI and NBR.&lt;/p&gt;

&lt;p&gt;Pre-fire vegetation experiences high NIR reflectance and low SWIR reflectance, while post-fire vegetation experiences low NIR reflectance and a high SWIR response. The region’s overall high dNBR values illustrate an intense burning, with seldom areas experiencing negative values and the associated increase in vegetation regrowth. Seasonal variations were outside the scope of this research, due to the relatively small change in spectral signatures during California’s dry period (from July to August). As predicted, Mendocino County experienced high NDVI values prior to the Complex, and a substantial decrease in NDVI values after the Complex. NDVI values prior to the Complex on July 10th were moderately high, taking into account the effect of California’s Mediterranean climate. The lack of precipitation in the year 2018, coupled with intensified temperature values during the dry summer season, explain for the pre-complex mean NDVI value of 0.33, post-complex mean NDVI value of 0.08, and total change mean NDVI value of -0.24.&lt;/p&gt;

&lt;p&gt;This research is imperative for understanding vegetation change as a result of fire disturbances in a Mediterranean climate. With an almost guarantee of increased global carbon dioxide levels (and ergo global temperature values) from global warming, the emergence of fire disturbances like wildfires will ensue and increase. Because of this realization, the intensified studying of NDVI changes to such phenomena will ensure more careful monitoring towards vegetation in the years to come. Likewise, the development of an enhanced burn severity index other than NBR may be needed. In essence, this newly developed burn severity index would study the possibility of ambiguities in reflectance data during the data preprocessing stage of remote sensing techniques.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements:&lt;/h3&gt;

&lt;p&gt;I would like to personally thank Dr. Nick Burkhart for being my advisor during the curation of my independent research. I would also like to thank Geoffrey Fricker for sharing his knowledge of advance remote sensing techniques, and his continued interest in my academic and professional fields related to GIS.&lt;/p&gt;

&lt;p&gt;Sources: GeoMac, USGS Earth Explorer, USGS UN-SPIDER, USDA National Forest, US Census Bureau 2018 TIGER/ Line, the California Natural Resources Agency National Hydrology Dataset (NHD).&lt;/p&gt;

</description>
        <pubDate>Thu, 20 Sep 2012 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2012/09/20/Mendocino-Fire-Complex/</link>
        <guid isPermaLink="true">http://localhost:4000/2012/09/20/Mendocino-Fire-Complex/</guid>
        
        <category>Remote Sensing</category>
        
        
      </item>
    
      <item>
        <title>California's Most Destructive Wildfire: Mapping The Tubbs Fire (2017)</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract:&lt;/h3&gt;

&lt;p&gt;Using remotely sensed images for at-risk areas - like those entrenched in wildfires - is essential for monitoring vegetation indices and mitigating the fire’s destructive spread. This project examines the Tubbs Wildfire that devastated Sonoma County, CA in October of 2017. Three objectives were set for the parameters of this project’s success: measuring active and burnt area extent using thermal imagery, comparing NDVI fluctuations for vegetation health with fire activity, and illustrating post-fire social and structural implications for Sonoma County. This presentation was culminated in tandem with the following peers from the University of California, Los Angeles: Shannon Cavanaugh, Austin Gates, Mikayla Hart, Jaclyn Villars Klaus, and Madeline Jordan.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction:&lt;/h3&gt;

&lt;p&gt;California’s Meditterranean climate makes it susceptible to “fire seasons” - large quantities of rainfall are collected in the winter, followed by a hot and dry summer that essentially leaves blooming vegetation left to dry out and become fuel for fires. In addition to drying vegetation, wind systems such as the Diablo and Santa Ana winds circulate through Northern and Southern California. The Diablo winds in Northern California bring hot, dry air that originate offshore due to high pressure over the land and low pressure over the ocean. The Santa Ana’s in Southern California bring similar hot, dry winds that stem from high pressures inland. The prevalence of these wind circulations is felt strongest during the months of September through December - now denoted as California’s “fire season.”&lt;/p&gt;

&lt;p&gt;One of the most damaging fires in California was the Tubbs Fire (2017). The fire started on October 8th and quickly spread throughout Sonoma County and Napa County, burning a total of 36,807 acres until its full containment on October 31st. To accurately assess the breadth of the Tubb’s Fire, critical hotspots were georeferenced and thermal anomalies were detected using Landsat8’s thermal infrared band 10. This is an effective method in mapping a fire’s intensity and burn severity, because the thermal emissivity of an active fire is greater than that of its surroundings.&lt;/p&gt;

&lt;p&gt;The Normalized Difference Vegetation Index (NDVI) is widely used to detect and measure the extent of wildfires by using the photosynthetic activity of vegetation as an indicator of a fire’s emersion. NDVI uses the Near Infrared and Red spectral bands to measure healthy vegetation based on the principle that photosynthetic organisms absorb most Red light and reflect most Near Infrared light. NDVI values in burned areas were found to be significantly lower than unburned areas. In this study, MODIS sensors were used to compare the change in NDVI during the Tubbs Fire with average annual values from the same region.&lt;/p&gt;

&lt;p&gt;It is expected that the thermal bands will provide an accurate active fire extent and allow sensors to identify critical areas of the fire, as the extreme temperature of fire should provide a radiant flux easily distinguishable from the surrounding landscape. It is also expected that lower NDVI values represent lower vegetation production as burned vegetation will no longer reflect Near Infrared light or absorb Red light to the comparison of healthy vegetation. It is anticipated that the recovery effort will be difficult and slow in the Santa Rosa region, due to the sheer number of structures destroyed (5,100).&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods:&lt;/h3&gt;

&lt;p&gt;Before explaining the process in its entirety, it’s imperative to first understand the basics of thermal remote sensing. Every object on Earth with an internal temperature greater than absolute zero emits unique thermal infrared energy, known as Radiant Flux. Thermal infrared energy extends from 3-14μm on the spectral scale. Unfortunately, we are not able to utilize this full range; atmospheric interference is a limited factor. H20, CO2, and O3 form absorption bands that preoccupy a significant amount of thermal infrared radiation between 3-14μm. This creates atmospheric windows in the electromagnetic spectrum for sensors. The Earth’s ozone layer absorbs much of the thermal energy exiting the terrain in an absorption band from approximately 9 - 10μm. This leaves the window from 10.5 - 12.5μm to avoid the absorption band. We will utilize Landsat8’s Band 10 which occurs from 10.6 - 11.19μm.&lt;/p&gt;

&lt;p&gt;In terms of cost and resolution, the best imagery accessible was Landsat 8 OLI/TIRS C1 Level-1. This sensor provides 11 bands at 30m spatial resolution, capturing the entirety of Earth every 16 days. The sensor follows a near polar orbit, capturing 400 scenes per day and completing over 14 orbits. This both reduces the risk of cloud interference and creates the possibility of scene overlap for analysis purposes. Landsat 8 thermal infrared is acquired at 100m resolution and resampled to 30m resolution. Although Landsat 8 provides two thermal bands, Bands 10 and 11, users should refrain from relying on Band 11 data for quantitative analysis. Band 11 adheres to larger calibration uncertainty, contaminated by stray light from outside the field of view. Using Glovis, a product by USGS, this study was able to come to fruition through the comparison of two scenes captured on October 11, 2017 and October 12, 2017.&lt;/p&gt;

&lt;p&gt;Temperature threshold were created by comparing the Digital Number (DN) pixel values in the area we knew the Tubbs Fire scorched with DN pixel values of the surrounding, unscathed area. To isolate active fire pixels from the rest of the scene, the threshold of DN values set for October 11th was 31,750 and the threshold of DN values set for October 12th was 24,750. These values were converted to represent a range approximating 290K-360K using the Temperature Retrieval Algorithm provided by the Landsat website. This range represents “Top of Atmosphere (TOA) Brightness Temperature,” not Land Surface Temperature. Converting TOA Brightness Temperature to Land Surface Temperature is possible, but often provides inaccurate estimation due to water vapor interference and emissivity.&lt;/p&gt;

&lt;p&gt;Once the thresholds for both scenes were finalized, the images obtained on October 11, 2017 and October 12, 2017 were imported into Arcmap. The images were reclassified, and a new raster was created with all pixel values under the threshold parameters ascribed ‘null.’ Polygons representing DN values greater than the thresholds were produced using a raster-to-vector conversion. These polygons, labeled ‘Thermal Anomalies,’ were then overlaid on both original thermal images.&lt;/p&gt;

&lt;p&gt;In order to assess the impact the Tubbs Fire had on vegetation, our study needed a large range of NDVI values during and after the fire with the norms for the same time span in previous years. The product eMODIS V6 NDVI was selected to perform calculations and evaluate the annual comparison. The MODIS vegetation indices were calculated based on 16-day intervals of daily assessments with Red and Near Infrared Bands. They were also corrected for Bidirectional Reflectance Distribution Function (BRDF). Imagery was downloaded in 1-week intervals from USGS during the timeframe of September 26th through November 20th. The 8 images produced from this timeline were clipped with the shapefile of the Tubbs Fire’s extent. Zonal Statistics were calculated to acquire the median, mean, minimum, and maximum NDVI values for each week. The median values were used as the point of reference, determining the change each week. A choropleth map was chosen to visually represent the Tubbs Fire’s effect on vegetation over time.&lt;/p&gt;

&lt;p&gt;To confirm the correlation between decreasing NDVI values with an increasing Tubbs Fire extent (and not just normalcy of decreasing vegetation for this time of year), median NDVI values for the same 8-week period were calculated for the year prior (2016) using the same process. Line charts comparing the ‘Weekly Median NDVI Value’ for September 26 - November 20 were created to compare the change in NDVI before the Tubbs Fire (2016) and during the Tubbs Fire (2017). To further cement this correlation, this process was done for the same weeks for the years 2014 and 2015. A declining slope of the trendline illustrates a decrease in NDVI, while a positive slope denotes an increase in NDVI.&lt;/p&gt;

&lt;p&gt;In order to gauge recovery efforts of the Tubbs Fire, demographic maps were produced to showcase the rapid movement of the fire from the less populated wine country of Napa County to the densely populated, urbanized region of Sonoma County. The two demographics of interest for the study were population and income. The Tubbs Fire polygon was layered with demographic data provided from the 2010 US Census.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results:&lt;/h3&gt;

&lt;h3 id=&quot;landsat-thermal-imagery---october-11th-2017&quot;&gt;Landsat Thermal Imagery - October 11th 2017&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tubbs-map-1.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ‘Thermal Anomalies’ in the map above represent DN pixels greater than the threshold set for the image captured on October 11, 2017 (31,750). The TOA Brightness Temperature equivalent to this threshold ranges from 306.25K - 367.12K.&lt;/p&gt;

&lt;h3 id=&quot;landsat-thermal-imagery---october-12th-2017&quot;&gt;Landsat Thermal Imagery - October 12th 2017&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tubbs-map-2.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ‘Thermal Anomalies’ in the map above represent DN pixels greater than the threshold set for the image captured on October 12, 2017 (24,750). The TOA Brightness Temperature equivalent to this threshold ranges from 289.55K - 357.52K.&lt;/p&gt;

&lt;h3 id=&quot;total-area-burned&quot;&gt;Total Area Burned&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tubbs-map-3.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The map above represents the burned area of the Tubbs Fire’s extent at its apex - October 10th through October 15th. Shapefiles of the boundaries were provided from a Cal Fire Server. The polygons were then separated by date to illustrate a 5-day choropleth map.&lt;/p&gt;

&lt;h3 id=&quot;ndvi-analysis---tubbs-fire-boundary&quot;&gt;NDVI Analysis - Tubbs Fire Boundary&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tubbs-ndvi.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The composite of 8 maps above illustrate a side-by-side comparison of the weekly change in NDVI values for the Tubbs Fire’s extent between September 26, 2017 and November 20, 2017.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/weekly-median-ndvi.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In 2016, a year before the Tubbs fire occurred, the extent showcased a slight increase in vegetation health (NDVI values). This increase in vegetation health is heavily juxtaposed by the following year’s (2017) indices. As expected, the fire catalyzed an ongoing regression of vegetation health (characterized by the prominent downward trend in NDVI values).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/yearly-median-ndvi.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To add another facet of understanding to our analysis, a calculation of vegetation health in the years 2014 and 2015 was further examined. This analysis served as a safety precaution of sorts: if the vegetation health of the extent (multiple years prior to the Tubbs fire’s outbreak) showed an apparent decrease, there had to be other outside factors influencing these oscillations. However, these prior years followed the same trajectory of 2016 — an overall increase in vegetation health. Ergo, the clear decrease in NDVI values in the year 2017 was an outcome of the Tubbs fire’s devastation.&lt;/p&gt;

&lt;h3 id=&quot;income-analysis&quot;&gt;Income Analysis&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tubbs-income.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This illustration represents the Mean Annual Income of areas impacted by the Tubbs Fire. An income analysis helps aide the recovery efforts placed in order after the Tubbs Fire was fully contained. The burned area in the figure above indicates a considerably financially stable representation of people affected. Only two of thirteen census tracts were represented as ‘Red’ with a mean annual income lower than $58,000. The national mean average income is approximately $51,939 so the two tracts within the lowest income range exceeds that of the national average. Five of thirteen census tracts denote incomes greater than $100,000 - which only 20% of the American population makes. Understanding the wealth in the affected area helps assume higher rates of recovery; most people within this income ranges can afford home insurance and other amenities that will likely make recovery easier. If the Tubbs Fire had occurred in an area with substantially lower annual income averages, it is probable that longer rates of recovery and rebuilding would occur.&lt;/p&gt;

&lt;h3 id=&quot;population-density-analysis&quot;&gt;Population Density Analysis&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tubbs-population.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The map above illustrates the Tubbs Fire’s spread from a less populated region (Napa County) to a more clustered population in Sonoma County. This emphasizes the amount of people affected by the Tubbs Fire’s destruction, and the reason it was coined ‘California’s Most Destructive Wildfire’ in 2017.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion:&lt;/h3&gt;

&lt;p&gt;After reviewing the data, it was unanimous that while Landsat 8 certainly allows for the identification of thermal anomalies such as wildfires, perhaps an AVIRIS-like hyperspectral sensor would have been more accommodating for this analysis. There are many limitations to remotely sensing wildfires thermally - including spatial, temporal, and spectral resolutions, as well as interference from scattering and emissivity. It is important to remember that Landsat 8 TIRS bands are acquired at 100m resolution but are then resampled to 30m. While the spatial resolution provides a mean area of estimation, the precision of the active fire area is jeopardized by the resampled area - which could provide misconstrued data for fire crews in their attempt to mitigate the Tubbs Fire. Temporally, one or two scenes every 16 days will not allow daily mapping of the active fire regions. And spectrally, the small atmospheric windows sensing thermal infrared energy are limiting.&lt;/p&gt;

&lt;p&gt;Touched upon above, interference and scattering also affect sensors outside Earth’s atmosphere. In addition to the atmospheric windows an orbital sensor must work within, the amount of smoke produced by fires hinder a thermal sensor’s ability to accurately sense an active fire’s area. Due to the large diameter of smoke particles, Mie Scattering occurs - when the diameters of atmospheric particles are so similar to the wavelength of scattering light that the thermal infrared wavelengths do not reach the sensors. Cloud cover can also alter data, as water vapor non selectively scatters all wavelengths and limits the sensor’s ability to collect data. There may also be light contamination in the thermal infrared bands.&lt;/p&gt;

&lt;p&gt;Regardless if a hyperspectral sensor like AVIRIS was equipped to a geostationary satellite with finer spatial resolution (10m-20m), there will always be a variable impeding thermal measurements. Interference and scattering from clouds, smoke, and atmospheric gas will always be considered when performing calculations and analyses.&lt;/p&gt;

</description>
        <pubDate>Wed, 19 Sep 2012 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2012/09/19/Tubbs-Fire/</link>
        <guid isPermaLink="true">http://localhost:4000/2012/09/19/Tubbs-Fire/</guid>
        
        <category>Remote Sensing</category>
        
        
      </item>
    
      <item>
        <title>Urban Applications of Supervised Classification: Landsat vs. MODIS</title>
        <description>&lt;p&gt;This project underscores the necessity of using high spatial resolution sensors when mapping urban centers, by contrasting a fine-resolution Landsat8 image with a medium-resolution MODIS image in Los Angeles. This exploration of using different spatial resolutions is coupled with the introduction of Supervised Classification, as we examine varying land cover types embedded in an urban setting.&lt;/p&gt;

&lt;p&gt;Supervised Classification allows more autonomy than its counterpart, Unsupervised Classification. A Supervised Classification gives the creator the ability to designate specific Regions of Interest (ROIs) in the image that correspond to different land cover types. The user can then specify the amount of pixel values or spectral indices for each class.&lt;/p&gt;

&lt;h3 id=&quot;obtaining-the-data&quot;&gt;Obtaining the Data:&lt;/h3&gt;

&lt;p&gt;12 Landsat images (Landsat8 OLI) were obtained through the USGS Global Visual Viewer (Glovis), with the approximate coordinates set for Los Angeles (LAT: 34, LON: -118). The time period isolated was November 5th, 2013.&lt;/p&gt;

&lt;p&gt;The MODIS images were provided by NASA Reverb. An 8-day composite was selected (MODIS Terra Surface Reflectance 8-Day L3 Global 250m). MODIS imagery is available in 250m resolution - the one selected for this study - with 2 bands available: Near-Infrared (NIR) and Red. MODIS imagery is also available in 500m resolution with 6 bands. I decided to sacrifice having a higher spectral resolution for a finer spatial resolution and used 2 bands (NIR &amp;amp; Red) for the classification. The 8-day composite downloaded corresponds best with the Landsat November 5th image, taken between the dates of November 1st and November 8th.&lt;/p&gt;

&lt;p&gt;Comparing the pixel values for Band 1 and Band 2 - as well as units, wavelength range, spectral signature, and spatial resolution - between the Landsat image and MODIS image is imperative for the success of this project. Through careful analysis, it was decided that Landsat’s Band 4 (Red) and band 5 (NIR) were most comparable with the 250m MODIS data.&lt;/p&gt;

&lt;h3 id=&quot;preparing-data-for-classification---landsat-vs-modis&quot;&gt;Preparing Data for Classification - Landsat vs MODIS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Supervised-Landsat.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Supervised-Modis.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first step in preparing the Landsat image for classification was stacking Band 4 and Band 5 together. The next step was to subset our newly stacked image. The general Los Angeles area needed to have prominent features - like the Bob Hope Airport (North West Corner), Rose Bowl Stadium (North East Corner), Santa Monica Pier (South West Corner), and LAX (South East Corner). A Polygon ROI including these 4 features was created, and the data was subset for the ROI.&lt;/p&gt;

&lt;p&gt;The newly subset Landsat image was loaded using RGB = 5,4,4. The MODIS image was re-projected, subset to the same region as the Landsat image, and loaded with the band combination RGB = 2,1,1.&lt;/p&gt;

&lt;p&gt;The dominant colors in the image above are varying shades of red - some appearing as a darker maroon, others a bright red. Red seems to denote the vegetation in the ROI, the brightest red seen in the image above signifying agricultural fields and golf courses. There are also shades of white and grey spread intermittently throughout the image.&lt;/p&gt;

&lt;h3 id=&quot;supervised-classification---maximum-likelihood-landsat-vs-modis&quot;&gt;Supervised Classification - Maximum Likelihood (Landsat vs MODIS)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Maximum-Likelihood-Landsat.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Maximum-Likelihood-Modis.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Supervised Classification used was ‘Maximum Likelihood,’ which assumes that the statistics for each class in each band are normally distributed. The computer algorithm then calculates the probability that a given pixel belongs to a specific class. Each pixel is assigned to the class with the highest probability (the maximum likelihood).&lt;/p&gt;

&lt;p&gt;The remaining pixels in the image are classified by the Supervised Classification algorithm using training ROIs - which the user designates as the representation of each land cover class. 5 land cover classes were created for the images: Urban, Ocean, Mountains, Golf Courses, and Sand. The variation in number of ROIs created determine how many land cover types will be classified in the image. The more specific the ROI, the better the classification. A careful balance between covering enough pixels to spectrally represent each land cover type - and covering too large of an area that you lose specificity - must be maintained when classifying the ROI.&lt;/p&gt;

&lt;p&gt;Training ROIs were created for the images according to their band combinations - RGB = 4,3,3 (Landsat) and RGB = 2,1,1 (MODIS). When setting up the Maximum Likelihood Classification, regions were designated as separate classes and the Probability Threshold was set to ‘None.’ These classifications were then overlayed onto the originally stacked image for evaluation. When the land cover types for both Landsat and MODIS images matched, statistics and histograms were computed.&lt;/p&gt;

&lt;p&gt;Landsat achieves a much higher resolution for identifying nuanced areas, such as urban dwellings. This is why Landsat is often used for spatial objects. MODIS, on the other hand, would be preferred when analyzing temporal settings - like a daily land cover change among urban dwelling. The largest visible difference of classification between the Landsat and MODIS image was between mountain ranges. This may be because the Supervised Classification over-classified these large regions - it denoted general areas as ‘mountains’ that were nearby roads or urban dwellings.&lt;/p&gt;

&lt;h3 id=&quot;histogram-and-statistics-for-classification---landsat-vs-modis&quot;&gt;Histogram and Statistics for Classification - Landsat vs. MODIS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/landsat-vs-modis-stats.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chart above uses statistics and histograms to showcase the two differing sensors’ ability to classify land cover type for the subset ROI. Landsat 8 had a mean of 2.88, with a standard deviation of 1.16. MODIS had a mean of 3.70, with a standard deviation of 1.22. The DN associated with each land cover type is shown through the histogram breakdown of the chart. The final portion of the chart - “Total Area Change” - is calculated by subtracting the area from Landsat 8 classification with the area from MODIS classification. Negative values, like those for Golf Course, Ocean, and Sand, represent a gross overestimation of land cover type by the differing sensors.&lt;/p&gt;

</description>
        <pubDate>Tue, 18 Sep 2012 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2012/09/18/Supervised-Classification/</link>
        <guid isPermaLink="true">http://localhost:4000/2012/09/18/Supervised-Classification/</guid>
        
        <category>Remote Sensing</category>
        
        
      </item>
    
      <item>
        <title>Water Resource Analysis: Lake Change Detection of the Tibetan Plateau</title>
        <description>&lt;p&gt;This project accentuates one of Remote Sensing’s most important facets: Change Detection Mapping. More specifically, a water resource analysis of lake change detection. The ROI is the Tibetan Plateau - one of the world’s most vulnerable areas to global change. There is a pronounced temperature rise of 0.16°C every decade. It’s clear the Tibetan lakes are experiencing alternations in their distribution and inundation area; however, the Tibetan Plateau’s inhospitable environment and general lack of accessibility have left lake dynamics shrouded in mystery.&lt;/p&gt;

&lt;p&gt;Landsat imagery taken over the course of 25 years were used to map and detect multi-decadal changes of a particular lake in the Tibetan Plateau: Dawa Co. This small, saline lake is approximately 115km and located in the heart of the Tibetan Plateau. A much smaller satellite lake (roughly 8km) is viewable East of Dawa Co. A series of images were acquired during the same season by 3 different Landsat sensors: Landsat MSS (1976), Landsat TM (1990), and Landsat ETM+ (2000).&lt;/p&gt;

&lt;h3 id=&quot;preparing-the-image&quot;&gt;Preparing the Image:&lt;/h3&gt;

&lt;p&gt;A Region of Interest (ROI) was created around the Dawa Co. and satellite lake. The Landsat MSS image was subset using the ROI and acted as our base of reference - a template of sorts for later subsetting the Landsat TM and Landsat ETM+ images. Aggregating and Subsetting the Landsat TM and Landsat ETM+ images were essentially for performing lake change detection. Landsat TM and ETM+ images need resampling because their images are a much finer resolution than Landsat MSS. To compromise with the much coarser spatial resolution of the Landsat MSS image, the pixel sizes of Landsat TM and ETM+ were resized and resampled to match the pixel size of the Landsat MSS image.&lt;/p&gt;

&lt;p&gt;Because these varying sensors offer different perspectives of the ROI, adjustments in band ratios need to be performed. For instance, both Landsat MSS and Landsat ETM+ images showcase the Northern 1/3rd of the satellite lake very light in color. This is due to the sensors picking up high amounts of salt sediment on the bottom of the lake. The Landsat ETM+ image, on the other hand, shows little of this salty bottom. Band Ratio Adjustment is performed to reduce this bottom effect using the following formula: &lt;strong&gt;(b1x100/(b2+1))&lt;/strong&gt;. The variable b1 in the formula is defined as Band 4, and b2 is ascribed to Band 3. This readjustment was performed for all Landsat sensors.&lt;/p&gt;

&lt;p&gt;At the heart of this Lake Change Detection project was defining the Spectral Threshold for water. The lakes from the surrounding land mass in the ratio image were extracted, and histograms were created for the ratio Landsat TM, MSS, and ETM+ images. These histograms helped define the data value threshold break between water and land for the images.
The Estimated Threshold Breaks for the sensors were as followed:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Landsat TM: 61&lt;/li&gt;
  &lt;li&gt;Landsat MSS: 54&lt;/li&gt;
  &lt;li&gt;Landsat ETM+: 64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Change Detection is visually represented through a classification scheme - operations achieved through ‘Color Mapping’ and ‘Density Slicing.’ The original range was deleted, and new ranges were added that span from 0 to the Threshold Value of the image. Post-Classification Corrections were also performed on the image, due to lake area’s containing isolated pixels (‘speckle’). These were removed.&lt;/p&gt;

&lt;p&gt;Once the images were corrected, they were combined together into one composite RGB image. This allows for an easy visualization of the change detection between each image.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Landsat MSS image was set to ‘R’&lt;/li&gt;
  &lt;li&gt;Landsat TM image was set to ‘G’&lt;/li&gt;
  &lt;li&gt;Landsat ETM+ image was set to ‘B’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A Linear Stretch was applied to the stacked image for a proper representation - at first the image displays as black. Statistics and Histograms are calculated for each of the three final classifications.&lt;/p&gt;

&lt;h3 id=&quot;change-detection-mapping&quot;&gt;Change Detection Mapping&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lake-detection.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The colors illustrated in the RGB composite image above showcase the change in the lakes’ size throughout the years (1976, 1990, and 2000). The lakes’ extent in the first year (1976) is denoted by the Red coloring (Landsat MSS - R:1, G:0, B:0). The lakes’ extent in the second year (1990) is denoted by the Yellow coloring (Landsat TM - R:1, G:1, B:0). The lakes’ extent in the final year (2000) is denoted by the White coloring (Landsat ETM+ : R:1, G:1, B:1).&lt;/p&gt;

&lt;p&gt;The value of “1” changes in accordance to which layer is placed on top of one another. This is why the lakes’ extent in the most recent year (2000) has a value of “1” for the three RGB Bands.&lt;/p&gt;

&lt;p&gt;Through Change Detection Mapping, we can deduce that Dawa Co. and its surrounding satellite lake are experiencing different directions of change - in fact, they’re inverses of each other. Dawa Co. is decreasing in size, while the satellite lake is increasing. This difference in size is illustrated by which colors are categorized by the value “1” for RBG. The unanimous RBG value of “1” (R:1, G:1, B:1) for the satellite lake solidifies its increasing size. The furthest extent is represented by the Dark Blue area, with values of R:0, G:0, B:1. The middle extent is categorized by the Light Blue area, with values of R:0, G:1, B:1. The shortest extent is denoted by the White area, with values of R:1, G:1, B:1.&lt;/p&gt;

&lt;p&gt;The dark blue area denotes the furthest extent, its value being R:0, G:0, B:1. The value categorized as B, ETM+, denotes the most recent image of the lake in year 2000. The light blue area denotes the middle extent of the lake, and its size is determined by the TM 1990 image (its value being R:0, G:1, B:1). The satellite lake’s smallest white extent is categorized with the RGB value: R: 1, G:1, B:1. This indicates that the extent was observed by the 3 satellite images listed above (TM, MSS, ETM+). Based on the values of these bands, the direction of the size appears to be increasing.&lt;/p&gt;

&lt;h3 id=&quot;statistics&quot;&gt;Statistics&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lake-stats.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The statistics above help understand Lake Pixel Area Change over the 25-year duration. This change was calculated between the 1976 - 1990 images and between the 1990 - 2000 images. The Landsat MSS (1976) image had a Npts of 37,102. Landsat TM (1990) has a Npts of 35,887. The difference between these values allows for a total Lake Pixel Area Change of 1,215 over the course of 1976 to 1990. Subtracting Landsat TM’s (1990) Npts of 35,887 with Landsat ETM+ (2000) Npts of 35,308 gave us the final Lake Pixel Area Change of 579. This lower Pixel Change makes sense, since the timeframe between the Landsat TM and Landsat ETM+ images are less than the 1976 - 1990 timeframe.&lt;/p&gt;

</description>
        <pubDate>Mon, 17 Sep 2012 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2012/09/17/Lake-Change-Detection/</link>
        <guid isPermaLink="true">http://localhost:4000/2012/09/17/Lake-Change-Detection/</guid>
        
        <category>Remote Sensing</category>
        
        
      </item>
    
      <item>
        <title>Analysis of Topographic Data: DEM of Mt. Whitney, CA using SRTM Imagery</title>
        <description>&lt;p&gt;This project unveils the prevalence in using 3D representations of the Earth’s surface: Digital Elevation Models (DEMs). Elevation data for MT. Whitney, CA was obtained through the Shuttle Radar Topography Mission (SRTM) - a mission that garnered a near-global scale, high-resolution digital topographic database of Earth. SRTM consisted of a specially modified radar system (C-Band and X-Band) that flew onboard the Space Shuttle Endeavor during an 11-day mission in February of 2000. As a result of this effort, three resolutions were produced: 30 arc second (1km), 3 arc second (90km), and 1 arc second (10m).&lt;/p&gt;

&lt;h3 id=&quot;preparing-the-image&quot;&gt;Preparing the Image:&lt;/h3&gt;

&lt;p&gt;The images produced below showcase DEM’s prevalence for 2 types of modeling: Elevation and Hydrological. These are seen through the creation of Elevation Profiles for MT. Whitney - the highest summit in the contiguous U.S. - and a Transect Profile for the largest river in its vicinity (Kern River).&lt;/p&gt;

&lt;p&gt;At the center of the DEM is the Kern River (N-S direction), with Owen’s Valley on the far right and MT. Whitney in-between the two. To highlight the highest elevations in the MT. Whitney area, a density slice was performed and overlayed on the original image. The default classification for the density slice was used; however, the ranges were customized to parallel the elevation of MT. Whitney. The range’s starting point was set to 4,000, and its ending point was set to MT. Whitney’s maximum elevation point.&lt;/p&gt;

&lt;p&gt;Elevation and Vertical Profiles were created to better understand the topography of MT. Whitney. An Elevation Transect was produced to get a closer look at the flow of water in the Kern River. A Hillshade Model was created to accentuate the elevation of MT. Whitney and grasp the sun’s impact on remotely sensed images.&lt;/p&gt;

&lt;h3 id=&quot;analyzing-horizontal--vertical-profiles---mt-whitney-ca&quot;&gt;Analyzing Horizontal &amp;amp; Vertical Profiles - Mt. Whitney, CA&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Topographic-Data-Profiles.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The horizontal and vertical profiles in the graphs above are particular to MT. Whitney’s topographical layout. They showcase the oscillations in elevation throughout a continuous distance, sharing the same topography. An analysis between the two profiles uncovers the relationship between elevation and distance. The horizontal profile indicates a decrease in elevation the further the distance from MT. Whitney’s peak. The vertical profile proves its counterpart - an increase in elevation the closer to MT. Whitney’s peak.&lt;/p&gt;

&lt;h3 id=&quot;elevation-transect---kern-river&quot;&gt;Elevation Transect - Kern River&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Elevation-Transect.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image extracted from SRTM is helpful in analyzing the elevation transect of the Kern River, because DEMs allow for easy extraction of drainage networks (hydrological modeling). Graphing the spatial profile of Kern River allows for a palpable representation of the water’s flow - Southward.&lt;/p&gt;

&lt;h3 id=&quot;considerations-for-sun-positioning---application-of-colored-hill-shade&quot;&gt;Considerations for Sun Positioning - Application of Colored Hill Shade&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Colored-Hillshade.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hill Shade models are created for consideration of possible topographic changes due to the sun’s positioning. The sun elevation angle is how high the sun is positioned in the sky. The sun azimuth angle describes which direction the sun is coming from - with North as 0° increasing clockwise to 360°. These two contrasting images of the same hill shade model illustrate the necessity in understanding the sun’s impact on DEMs.&lt;/p&gt;

&lt;p&gt;The hill shade model on the left represents an elevation angle set to 90° and an azimuth angle set to 360°. With these parameters, the sun’s setting is essentially on the horizon (the northern region). This is a plausible scenario for projecting the sun’s impact on a remotely sensed image, given the sun’s placement within a 90° incremental oscillation.&lt;/p&gt;

&lt;p&gt;The hill shade model on the right represents an elevation angle set to 10° and an azimuth angle set to 0°. These unrealistic parameters were set to emphasize the importance in comprehending practical scenarios for the sun’s positioning. The image on the right is dark in color - no sun in sight. This makes sense, because the sun doesn’t set in the North.&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Sep 2012 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2012/09/16/SRTM-Topographic/</link>
        <guid isPermaLink="true">http://localhost:4000/2012/09/16/SRTM-Topographic/</guid>
        
        <category>Remote Sensing</category>
        
        
      </item>
    
      <item>
        <title>Active Remote Sensing: Interpolation &amp; Analysis of SAR Imagery</title>
        <description>&lt;h3 id=&quot;analysis-of-sar-images--lidar-data---applications-of-a-gaussian-contrast-stretch&quot;&gt;Analysis of SAR Images &amp;amp; LiDAR Data - Applications of a Gaussian Contrast Stretch&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/active-rs.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This project emphasizes the strength in Active Remote Sensing - satellites capable of generating their own source of energy through electromagnetic radiation. The use of interpolation and analysis of SAR (Synthetic-aperture Radar) was used to obtain latitudinal and longitudinal coordinates for the image center.&lt;/p&gt;

&lt;p&gt;The application of a ‘Gaussian Contrast Stretch’ was used on the original image, which adds a gray filter to make darker pixels appear brighter and show less contrast. These contrasting representations of the same image highlight the basics of interpreting satellite imagery of urban centers. Rivers appear darker because a large portion of the radar’s energy is being reflected away from the sensor - the flat surface acting like a mirror. Buildings, on the other hand, appear almost overwhelmingly bright. This common occurrence is often referred to as the “Corner Reflection Effect.” When two smooth surfaces form a 90° angle that faces the beam shot from active satellites, it’s as if twice the energy is being reflected off the object and reflected back to the sensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Gaussian-histogram.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Histograms are useful graphic representations for remotely sensed images, because they allow for the analysis of a single band. Analyzing the input and output histograms from the Gaussian Contrast image (left) to the original image (right) allows for a heightened understanding of 3 statistical variables: Range, Central Tendency, and Distribution. In the realm of remote sensing, the Range refers to frequency of pixels and is always a value between 0 and 255. The Central Tendency is represented as a single value that denotes an entire dataset by its center. In this case, the Central Tendency for the Gaussian Stretch image is 122, while the 2% Linear Stretch of the original image is 29. The Distribution of the Gaussian Stretch image appears normal, while the 2% Linear Stretch of the original image is skewed to the right.&lt;/p&gt;

</description>
        <pubDate>Sat, 15 Sep 2012 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2012/09/15/Active-Remote-Sensing/</link>
        <guid isPermaLink="true">http://localhost:4000/2012/09/15/Active-Remote-Sensing/</guid>
        
        <category>Remote Sensing</category>
        
        
      </item>
    
      <item>
        <title>Unsupervised Classification: Land Cover Types in Los Angeles, CA</title>
        <description>&lt;p&gt;This project highlights the importance of using an Unsupervised Classification when dealing with vegetation. In particular, performing and interpreting Unsupervised Classification of differing land cover types in Los Angeles through Landsat 7 images. Unsupervised Classification is computer automated, with the user dictating the number of Classes represented, as well as Maximum Iterations (how many times the algorithm runs) and the Change Threshold Percentage (when the algorithm stops for the next classification to emerge). These classes represent the number of pixels for each band represented in the image. Clustering algorithms group the pixels together based on spectral similarities. When the classification is finalized, coloring is added to the image. This makes interpreting the classification easier for the viewer.&lt;/p&gt;

&lt;h3 id=&quot;preparing-the-images&quot;&gt;Preparing the Images:&lt;/h3&gt;

&lt;p&gt;Landsat 7 images were subset, recalibrated, and stacked. Spatially subsetting the image allows for a specific portion to be analyzed at a more granular level. Landsat 7 images allow for the RGB combination (bands 4,3,2) to come to the forefront of our analysis. There is heavy emphasis on band 4 (red) in the Landsat image, as we will be taking a closer look at Vegetation Indices (NDVI). Recalibrating the image is crucial for minimizing the path radiance for particular bands. Statistic parameters are computed by running simple statistics on the original bands and subset bands. Each of these newly recalibrated bands are then ‘stacked’ together with the subset image bands into a single image.&lt;/p&gt;

&lt;h3 id=&quot;ndvi-analysis&quot;&gt;NDVI Analysis&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unsupervised-ndvi.png&quot; alt=&quot;Map GIS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Understanding NDVI (Normalized Difference Vegetation Index) is paramount for properly interpreting an area’s vegetation cover. NDVI is calculated by subtracting the Near-Infrared Band (NIR) with the Red Band, and then dividing that total with the value obtained from adding the NIR Band with the Red Band. Brighter pixel values in the image above correspond to higher rates of photosynthesis - in essence, the brighter the pixel equates to the greater amount of ‘healthy’ vegetation.&lt;/p&gt;

&lt;h3 id=&quot;isodata-algorithm&quot;&gt;Isodata Algorithm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/isodata.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The computer automated Isodata Algorithm for Unsupervised Classification calculates class means evenly distributed in the data space. It then iteratively clusters the remaining pixels using minimum distance techniques. Each iteration recalculates the mean and reclassifies pixels with respect to the new means. This process continues until the number of pixels in each class changes by less than the selected pixel change threshold (or until the maximum number of iterations is reached).&lt;/p&gt;

&lt;p&gt;Our stacked, subset image was set to the following parameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A minimum class value of 4 and maximum class value of 6.&lt;/li&gt;
  &lt;li&gt;Maximum number of iterations set to 6.&lt;/li&gt;
  &lt;li&gt;Threshold set to 5.&lt;/li&gt;
  &lt;li&gt;Minimum number of pixels per class set to 1,000.&lt;/li&gt;
  &lt;li&gt;Standard Deviation set to 1.&lt;/li&gt;
  &lt;li&gt;Minimum class distance set to 6.&lt;/li&gt;
  &lt;li&gt;Max number of merge pairs set to 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To understand what land cover feature each class represents, a classified image showcasing an array of colors is overlayed on top of the stacked image. Countless experiments of re-running the classification is performed until the best classification of the area is developed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unsupervised-class.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Red in the image (Class 1) predominately illustrates bodies of water - lakes and oceans. The small portion of Red scattered among the Green denotes areas of extremely low net productivity.&lt;/p&gt;

&lt;p&gt;The Green in the image (Class 2) showcases areas of extremely high vegetation. These are most likely wilderness areas and areas with high productivity such as the Santa Monica Mountains and those near Santa Clarita.&lt;/p&gt;

&lt;p&gt;The Blue in the image (Class 3) represents lighter areas of vegetation - like small agricultural fields, or shrubs and bushes.&lt;/p&gt;

&lt;p&gt;The Yellow in the image (Class 4) emblemizes the urban areas in Los Angeles - a city dominated with concrete. Things like buildings, roads, parking lots, etc.&lt;/p&gt;

&lt;p&gt;The Light Blue (Class 5) illustrates varying areas of urban land cover. The color is distributed throughout the image in varying quantities, leading us to believe they’re areas with buildings and relatively low regions of productivity.&lt;/p&gt;

&lt;h3 id=&quot;histogram--statistics&quot;&gt;Histogram &amp;amp; Statistics&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hist-1.png&quot; alt=&quot;Placeholder&quot; /&gt;
&lt;img src=&quot;/assets/images/hist-2.png&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Computing histograms and statistics is often the final part of the study, when the user is satisfied with the classification of the image. Histograms are efficient in showing the image’s most dominant classes (its frequency). The Y-Axis indicates this frequency by showcasing the amount of time it occurs, while the X-Axis indicates the data values (classes). The statistics shed light on the number of pixels within each class. The ‘DN’ (Digital Number) column in the image above represents each alternating class. The ‘Npts’ column illustrates the total pixels in each category (pixels per class). General statistics, like the mean and standard deviation, are also computed. The ‘Percent’ column shows the percentage of space taken up by each category.&lt;/p&gt;

</description>
        <pubDate>Fri, 14 Sep 2012 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2012/09/14/Unsupervised-Classification/</link>
        <guid isPermaLink="true">http://localhost:4000/2012/09/14/Unsupervised-Classification/</guid>
        
        <category>Remote Sensing</category>
        
        
      </item>
    
  </channel>
</rss>
